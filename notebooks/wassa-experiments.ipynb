{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import scipy.stats\n",
    "\n",
    "def evaluate(pred,gold):\n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "        \n",
    "        for line in gold_lines:\n",
    "            line = line.decode()\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:   \n",
    "                data_dic[int(parts[0])]=[float(line.split('\\t')[3])]\n",
    "            else:\n",
    "                raise ValueError('Format problem.')\n",
    "        \n",
    "        \n",
    "        for line in pred_lines:\n",
    "            line = line.decode()\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:  \n",
    "                if int(parts[0]) in data_dic:\n",
    "                    try:\n",
    "                        data_dic[int(parts[0])].append(float(line.split('\\t')[3]))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[int(parts[0])].append(0.5)\n",
    "                else:\n",
    "                    raise ValueError('Invalid tweet id.')\n",
    "            else:\n",
    "                raise ValueError('Format problem.')\n",
    "            \n",
    "            \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "         \n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                if(data_dic[id][0]>=0.5):\n",
    "                    gold_scores_range_05_1.append(data_dic[id][0])\n",
    "                    pred_scores_range_05_1.append(data_dic[id][1])\n",
    "            else:\n",
    "                raise ValueError('Repeated id in test data.')\n",
    "                \n",
    "      \n",
    "        # return zero correlation if predictions are constant\n",
    "        if numpy.std(pred_scores)==0 or numpy.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                    \n",
    "        spear_corr=scipy.stats.spearmanr(pred_scores,gold_scores)[0]   \n",
    "\n",
    "\n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                                    \n",
    "        spear_corr_range_05_1=scipy.stats.spearmanr(pred_scores_range_05_1,gold_scores_range_05_1)[0]           \n",
    "        \n",
    "      \n",
    "        return (pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1)\n",
    "    else:\n",
    "        raise ValueError('Predictions and gold data have different number of lines.')\n",
    "        \n",
    "def evaluate_lists(pred, gold):\n",
    "    if len(pred) == len(gold):\n",
    "        gold_scores=gold\n",
    "        pred_scores=pred         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "         \n",
    "            \n",
    "        for i in range(len(gold_scores)):\n",
    "            if(gold_scores[i]>=0.5):\n",
    "                gold_scores_range_05_1.append(gold_scores[i])\n",
    "                pred_scores_range_05_1.append(pred_scores[i])\n",
    "                \n",
    "        # return zero correlation if predictions are constant\n",
    "        if numpy.std(pred_scores)==0 or numpy.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                    \n",
    "        spear_corr=scipy.stats.spearmanr(pred_scores,gold_scores)[0]   \n",
    "\n",
    "\n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                                    \n",
    "        spear_corr_range_05_1=scipy.stats.spearmanr(pred_scores_range_05_1,gold_scores_range_05_1)[0]           \n",
    "        \n",
    "      \n",
    "        return np.array([pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1])\n",
    "    else:\n",
    "        raise ValueError('Predictions and gold data have different number of lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_path = \"/home/v2john/\"\n",
    "wassa_home = \"/home/v2john/WASSA-Task/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google news pretrained vectors\n",
    "wv_model_path = word_vector_path + \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter pretrained vectors\n",
    "wv_model_path_1 = word_vector_path + \"word2vec_twitter_model.bin\"\n",
    "wv_model_1 = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path_1, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = np.array(embedding)\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "wv_model_path_2 = word_vector_path + \"glove.twitter.27B.200d.txt\"\n",
    "wv_model_2 = loadGloveModel(wv_model_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 400 200\n"
     ]
    }
   ],
   "source": [
    "w2v_dimensions = len(wv_model['word'])\n",
    "w2v_dimensions_1 = len(wv_model_1['word'])\n",
    "w2v_dimensions_2 = len(wv_model_2['word'])\n",
    "print(w2v_dimensions, w2v_dimensions_1, w2v_dimensions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# bi_tokens = list(bigrams(word_tokenize(\"This is a sample sentence!!!\")))\n",
    "# for bi_token in bi_tokens:\n",
    "#     print(\" \".join(bi_token))\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "        [word for word in string.split()\n",
    "         if word not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synsetlist = list(swn.senti_synsets('super'))\n",
    "# print(synsetlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_str(string):  \n",
    "    string = html.unescape(string)\n",
    "    string = string.replace(\"\\\\n\", \" \")\n",
    "    string = string.replace(\"_NEG\", \"\")\n",
    "    string = string.replace(\"_NEGFIRST\", \"\")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string) # removing any twitter handle mentions\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\*\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" ,\", string)\n",
    "    string = re.sub(r\"!\", \" !\", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return remove_stopwords(string.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata and Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, intensity):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "            \"id: \" + self.id + \\\n",
    "            \", text: \" + self.text + \\\n",
    "            \", emotion: \" + self.emotion + \\\n",
    "            \", intensity: \" + self.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_training_data(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], clean_str(array[1]), array[2], float(array[3])))\n",
    "    return train_list\n",
    "            \n",
    "def read_test_data(training_data_file_path):\n",
    "\n",
    "    test_list = list()\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            test_list.append(Tweet(array[0], clean_str(array[1]), array[2], None))\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion = \"sadness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_file_path = \\\n",
    "    wassa_home + \"dataset/\" + \\\n",
    "    emotion + \"-ratings-0to1.train.txt\"\n",
    "test_data_file_path = \\\n",
    "    wassa_home + \"dataset/\" + \\\n",
    "    emotion + \"-ratings-0to1.dev.target.txt\"\n",
    "predictions_file_path = \\\n",
    "    wassa_home + \"predictions/\" + \\\n",
    "    emotion + \"-pred.txt\"\n",
    "gold_set_path = \\\n",
    "    wassa_home + \"dataset/gold-set/\" + \\\n",
    "    emotion + \"-ratings-0to1.dev.gold.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Intensity Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affect_intensity_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-AffectIntensity-Lexicon.txt\"\n",
    "\n",
    "def get_word_affect_intensity_dict(emotion):\n",
    "    word_intensities = dict()\n",
    "\n",
    "    with open(affect_intensity_file_path) as affect_intensity_file:\n",
    "        for line in affect_intensity_file:\n",
    "            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_int_array[2] == emotion):\n",
    "                word_intensities[word_int_array[0]] = float(word_int_array[1])\n",
    "\n",
    "    return word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_intensities = get_word_affect_intensity_dict(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "\n",
    "def get_emo_int_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in word_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(word_intensities[word])\n",
    "    \n",
    "    return poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec + GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(tweet, model, dimensions):\n",
    "    vector_list = list()\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vector_list.append(model[word])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if len(vector_list) == 0:\n",
    "        vec_rep = np.zeros(dimensions).tolist()\n",
    "    else:\n",
    "        try:\n",
    "            vec_rep = sum(vector_list) / float(len(vector_list))\n",
    "        except Exception as e:\n",
    "            print(vector_list)\n",
    "            print(e)\n",
    "            raise Exception\n",
    "\n",
    "    return vec_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_sentiwordnet = PolynomialFeatures(5)\n",
    "\n",
    "def get_sentiwordnetscore(tweet):\n",
    "    \n",
    "    tweet_score = np.zeros(2)\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        synsetlist = list(swn.senti_synsets(word))\n",
    "        \n",
    "        if synsetlist:\n",
    "            tweet_score[0] += synsetlist[0].pos_score()\n",
    "            tweet_score[1] += synsetlist[0].neg_score()\n",
    "            \n",
    "    sentiwordnetscore_list = poly_sentiwordnet.fit_transform(tweet_score.reshape(1, -1))[0].tolist()\n",
    "    \n",
    "    return sentiwordnetscore_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Emotion Presence Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "def get_affect_presence_list(emotion):\n",
    "    word_list = list()\n",
    "    \n",
    "    with open(sentiment_emotion_lex_file_path) as sentiment_emotion_lex_file:\n",
    "        for line in sentiment_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[1] == emotion and word_array[2] == '1'):\n",
    "                word_list.append(word_array[0])\n",
    "                \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = get_affect_presence_list(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_emotion_feature(tweet):\n",
    "    for word in word_list:\n",
    "        if word in tweet.split():\n",
    "            return [1.0]\n",
    "    \n",
    "    return [0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2/\" + \\\n",
    "    \"NRC-Hashtag-Emotion-Lexicon-v0.2.txt\"\n",
    "    \n",
    "def get_hashtag_emotion_intensity(emotion):\n",
    "    hastag_intensities = dict()\n",
    "    \n",
    "    with open(hashtag_emotion_lex_file_path) as hashtag_emotion_lex_file:\n",
    "        for line in hashtag_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[0] == emotion):\n",
    "                hastag_intensities[clean_str(word_array[1])] = float(word_array[2])\n",
    "                \n",
    "    return hastag_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_emotion_intensities = get_hashtag_emotion_intensity(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "\n",
    "def get_hashtag_emotion_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in hashtag_emotion_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(hashtag_emotion_intensities[word])\n",
    "    \n",
    "    return poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticon Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-unigrams.txt\"\n",
    "emoticon_lexicon_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-bigrams.txt\"\n",
    "emoticon_lexicon_pairs_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-pairs.txt\"\n",
    "pair_split_string = \"---\"\n",
    "    \n",
    "emoticon_lexicon_unigrams = dict()\n",
    "emoticon_lexicon_bigrams = dict()\n",
    "emoticon_lexicon_pairs = dict()\n",
    "\n",
    "def get_emoticon_lexicon_unigram_dict():\n",
    "    with open(emoticon_lexicon_unigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_unigrams\n",
    "\n",
    "def get_emoticon_lexicon_bigram_dict():\n",
    "    with open(emoticon_lexicon_bigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_bigrams\n",
    "\n",
    "def get_emoticon_lexicon_pairs_dict():\n",
    "    with open(emoticon_lexicon_pairs_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in emoticon_lexicon_pairs.keys():\n",
    "                    token_1_dict = emoticon_lexicon_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                emoticon_lexicon_pairs[token_1] = token_1_dict\n",
    "    \n",
    "    return emoticon_lexicon_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigram_dict = get_emoticon_lexicon_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_lexicon_bigram_dict = get_emoticon_lexicon_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_pairs_dict = get_emoticon_lexicon_pairs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(emoticon_lexicon_pairs_dict))\n",
    "# random_key = list(emoticon_lexicon_pairs_dict.keys())[0]\n",
    "# print(random_key)\n",
    "# print(emoticon_lexicon_pairs_dict[random_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emoticon_lexicon = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in emoticon_lexicon_unigram_dict.keys():\n",
    "            vector_list += emoticon_lexicon_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_bigram_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in emoticon_lexicon_bigram_dict.keys():\n",
    "            vector_list += emoticon_lexicon_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_pair_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        word_1 = clean_str(tokens[i])\n",
    "        if word_1 in emoticon_lexicon_pairs_dict.keys():\n",
    "            token_1_dict = emoticon_lexicon_pairs_dict[word_1]\n",
    "            for j in range(i, len(tokens)):\n",
    "                word_2 = clean_str(tokens[j])\n",
    "                if word_2 in token_1_dict.keys():\n",
    "                    vector_list += token_1_dict[word_2]\n",
    "                    counter += 1\n",
    "                    \n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_emoticon_lexicon_vector(tweet):\n",
    "    final_list = list()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_emoticon_lexicon_vector(tokens))\n",
    "    \n",
    "    # Adding bigram features\n",
    "    final_list.extend(get_bigram_sentiment_emoticon_lexicon_vector(tokens))\n",
    "    \n",
    "    # Adding pair features\n",
    "    final_list.extend(get_pair_sentiment_emoticon_lexicon_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticon Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_afflex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-unigrams.txt\"\n",
    "emoticon_afflex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-bigrams.txt\"\n",
    "    \n",
    "emoticon_afflex_unigrams = dict()\n",
    "emoticon_afflex_bigrams = dict()\n",
    "\n",
    "def get_emoticon_afflex_unigram_dict():\n",
    "    with open(emoticon_afflex_unigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_unigrams\n",
    "\n",
    "def get_emoticon_afflex_bigram_dict():\n",
    "    with open(emoticon_afflex_bigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_afflex_unigram_dict = get_emoticon_afflex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_afflex_bigram_dict = get_emoticon_afflex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emoticon_lexicon = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_emoticon_afflex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in emoticon_afflex_unigram_dict.keys():\n",
    "            vector_list += emoticon_afflex_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "\n",
    "def get_bigram_sentiment_emoticon_afflex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in emoticon_afflex_bigram_dict.keys():\n",
    "            vector_list += emoticon_afflex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_emoticon_afflex_vector(tweet):\n",
    "    final_list = list()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_emoticon_afflex_vector(tokens))\n",
    "    \n",
    "    # Adding bigram featunigram_list =ures\n",
    "    final_list.extend(get_bigram_sentiment_emoticon_afflex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-AffLexNegLex-v1.0/\" + \\\n",
    "    \"HS-AFFLEX-NEGLEX-unigrams.txt\"\n",
    "hashtag_affneglex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-AffLexNegLex-v1.0/\" + \\\n",
    "    \"HS-AFFLEX-NEGLEX-bigrams.txt\"\n",
    "    \n",
    "hashtag_affneglex_unigrams = dict()\n",
    "hashtag_affneglex_bigrams = dict()\n",
    "\n",
    "def get_hashtag_affneglex_unigram_dict():\n",
    "    with open(hashtag_affneglex_unigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_unigrams[clean_str(word_array[0])] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hashtag_affneglex_unigrams\n",
    "\n",
    "def get_hashtag_affneglex_bigram_dict():\n",
    "    with open(hashtag_affneglex_bigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_bigrams[clean_str(word_array[0])] = np.array([float(val) for val in word_array[1:]])\n",
    "\n",
    "    return hashtag_affneglex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigram_dict = get_hashtag_affneglex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_affneglex_bigram_dict = get_hashtag_affneglex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_hashtag_sent_affneglex = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_hashtag_affneglex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in hashtag_affneglex_unigram_dict.keys():\n",
    "            vector_list += hashtag_affneglex_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_hashtag_sent_affneglex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_bigram_sentiment_hashtag_affneglex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in hashtag_affneglex_bigram_dict.keys():\n",
    "            vector_list += hashtag_affneglex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_hashtag_sent_affneglex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_hashtag_affneglex_vector(tweet):\n",
    "    final_list = list()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_hashtag_affneglex_vector(tokens))\n",
    "    # Adding bigram features\n",
    "    final_list.extend(get_bigram_sentiment_hashtag_affneglex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-Lexicon-v1.0/HS-unigrams.txt\"\n",
    "hash_sent_lex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-Lexicon-v1.0/HS-bigrams.txt\"\n",
    "hash_sent_lex_pairs_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-Lexicon-v1.0/HS-pairs.txt\"\n",
    "pair_split_string = \"---\"\n",
    "\n",
    "hash_sent_lex_unigrams = dict()\n",
    "hash_sent_lex_bigrams = dict()\n",
    "hash_sent_lex_pairs = dict()\n",
    "\n",
    "def get_hash_sent_lex_unigram_dict():\n",
    "    with open(hash_sent_lex_unigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_unigrams\n",
    "\n",
    "def get_hash_sent_lex_bigram_dict():\n",
    "    with open(hash_sent_lex_bigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_bigrams\n",
    "\n",
    "def get_hash_sent_lex_pairs_dict():\n",
    "    with open(hash_sent_lex_pairs_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in hash_sent_lex_pairs.keys():\n",
    "                    token_1_dict = hash_sent_lex_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                hash_sent_lex_pairs[token_1] = token_1_dict\n",
    "    \n",
    "    return hash_sent_lex_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_unigram_dict = get_hash_sent_lex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_bigram_dict = get_hash_sent_lex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_pairs_dict = get_hash_sent_lex_pairs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_hash_sent_lex = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_hash_sent_lex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in hash_sent_lex_unigram_dict.keys():\n",
    "            vector_list += hash_sent_lex_unigram_dict[word]\n",
    "            counter += 1\n",
    "\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    \n",
    "    return poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "    \n",
    "def get_bigram_sentiment_hash_sent_lex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in hash_sent_lex_bigram_dict.keys():\n",
    "            vector_list += hash_sent_lex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    \n",
    "    return poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_pair_sentiment_hash_sent_lex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        word_1 = clean_str(tokens[i])\n",
    "        if word_1 in hash_sent_lex_pairs_dict.keys():\n",
    "            token_1_dict = hash_sent_lex_pairs_dict[word_1]\n",
    "            for j in range(i, len(tokens)):\n",
    "                word_2 = clean_str(tokens[j])\n",
    "                if word_2 in token_1_dict.keys():\n",
    "                    vector_list += token_1_dict[word_2]\n",
    "                    counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "    \n",
    "def get_sentiment_hash_sent_lex_vector(tweet):\n",
    "    final_list = list()\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_hash_sent_lex_vector(tokens))\n",
    "    # Adding bigram features\n",
    "    final_list.extend(get_bigram_sentiment_hash_sent_lex_vector(tokens))\n",
    "    # Adding pair features\n",
    "    final_list.extend(get_pair_sentiment_hash_sent_lex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n"
     ]
    }
   ],
   "source": [
    "training_tweets = read_training_data(training_data_file_path)\n",
    "\n",
    "with open(training_data_file_path + \".cleaned\", 'w') as cleaned_input_file:\n",
    "    for tweet in training_tweets:\n",
    "        cleaned_input_file.write(tweet.id + \"\\t\" + tweet.text + \"\\n\")\n",
    "\n",
    "score_train = list()\n",
    "tweet_train = list()\n",
    "for tweet in training_tweets:\n",
    "    tweet_train.append(tweet.text)\n",
    "    score_train.append(float(tweet.intensity))\n",
    "print(len(score_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_tweets(tweet_list):\n",
    "    vectors = list()\n",
    "\n",
    "    for i in range(len(tweet_list)):\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"Vectorizing tweet \" + str(i))\n",
    "        \n",
    "        x_vector = list()\n",
    "        \n",
    "        '''Pre-trained Word embeddings'''\n",
    "        x_vector.extend(get_word2vec_embedding(tweet_list[i], wv_model, w2v_dimensions))\n",
    "        x_vector.extend(get_word2vec_embedding(tweet_list[i], wv_model_1, w2v_dimensions_1))\n",
    "        x_vector.extend(get_word2vec_embedding(tweet_list[i], wv_model_2, w2v_dimensions_2))\n",
    "\n",
    "        '''NRC Emotion Intensity Lexicon'''\n",
    "        x_vector.extend(get_emo_int_vector(tweet_list[i]))\n",
    "\n",
    "        '''WordNet'''\n",
    "        x_vector.extend(get_sentiwordnetscore(tweet_list[i]))\n",
    "\n",
    "        '''NRC Sentiment Lexica'''\n",
    "        x_vector.extend(get_sentiment_emotion_feature(tweet_list[i]))\n",
    "        x_vector.extend(get_sentiment_emoticon_lexicon_vector(tweet_list[i]))\n",
    "        x_vector.extend(get_sentiment_emoticon_afflex_vector(tweet_list[i]))\n",
    "\n",
    "        '''NRC Hashtag Lexica'''\n",
    "        x_vector.extend(get_hashtag_emotion_vector(tweet_list[i]))\n",
    "        x_vector.extend(get_sentiment_hash_sent_lex_vector(tweet_list[i]))\n",
    "        x_vector.extend(get_sentiment_hashtag_affneglex_vector(tweet_list[i]))\n",
    "\n",
    "        vectors.append(x_vector)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing tweet 0\n",
      "Vectorizing tweet 100\n",
      "Vectorizing tweet 200\n",
      "Vectorizing tweet 300\n",
      "Vectorizing tweet 400\n",
      "Vectorizing tweet 500\n",
      "Vectorizing tweet 600\n",
      "Vectorizing tweet 700\n",
      "786\n",
      "1504\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorize_tweets(tweet_train)\n",
    "print(len(x_train))\n",
    "dimension = len(x_train[0])\n",
    "print(dimension)\n",
    "\n",
    "# with open(\"/tmp/dump.txt\", 'w') as dump_file:\n",
    "#     for i in range(len(x_train)):\n",
    "#         if dimension != len(x_train[i]):\n",
    "#             print(len(x_train[i]), i)\n",
    "#         dump_file.write(str(x_train[i]))\n",
    "#         dump_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.047851562, -0.15006511, -0.26098633, 0.20930989, -0.03938802, 0.2376302, -0.039550781, -0.068359375, 0.2273763, -0.13802083, -0.077880859, -0.27229819, 0.11686198, 0.23681641, 0.11832682, 0.34309897, 0.081624351, 0.18115234, 0.14644368, -0.22786458, 0.015950521, -0.0013020834, 0.076497398, -0.17871094, 0.012532552, -0.099283852, -0.021565756, 0.19010417, -0.043212891, -0.010848999, -0.31477866, 0.020670572, 0.0081990557, -0.087239586, -0.16243489, -0.078694664, -0.22867839, -0.02718099, -0.058390301, 0.066243492, 0.088378906, -0.054606121, 0.14648438, -0.24023438, 0.041610718, -0.06363932, -0.039253235, -0.12760417, -0.21303304, -0.06754557, -0.26692709, 0.044433594, -0.2849935, -0.043782551, 0.23665364, -0.080729164, -0.30664062, 0.12015788, 0.061360676, -0.32584634, 0.021484375, 0.21777344, -0.060709637, -0.023111979, -0.0077718101, 0.10026041, 0.10253906, -0.012207031, -0.072916664, 0.030512491, 0.012532552, -0.17185466, 0.15781657, -0.099609375, -0.15559895, -0.20930989, 0.18001302, 0.20963542, -0.12540691, 0.35742188, -0.012858073, 0.0052083335, -0.099527992, -0.17854817, -0.04239909, 0.083089195, -0.034790039, 0.29427084, -0.018880209, -0.25358072, -0.16927083, -0.06526693, -0.11197916, 0.12792969, -0.17122395, 0.053385418, 0.441569, -0.039225262, 0.1398112, 0.18261719, 0.042317707, 0.060546875, 0.076171875, -0.052001953, 0.11197916, -0.13037109, -0.27083334, -0.026245117, 0.088918053, -0.15852864, -0.099283852, -0.11572266, 0.14599609, -0.24511719, 0.27669272, 0.039749146, -0.11946615, -0.0071614585, 0.080729164, 0.16964722, -0.15494792, -0.009765625, -0.14713542, -0.04296875, -0.0032552083, -0.33398438, 0.056477863, -0.018880209, 0.15437825, 0.072916664, -0.04284668, -0.17740886, -0.14790852, 0.1360677, -0.043426514, -0.053995769, -0.065389, 0.0703125, -0.021972656, -0.0078125, -0.0032552083, -0.033203125, -0.10253906, -0.024739584, -0.034912109, -0.10042318, -0.2063802, 0.2220052, -0.042317707, -0.13346355, 0.23372395, -0.036458332, 0.059000652, 0.30631509, -0.19335938, -0.037760418, 0.064575195, -0.19010417, 0.059895832, -0.033671062, 0.14241536, 0.18461101, 0.081217445, 0.16015625, -0.041748047, -0.10760498, 0.055989582, 0.075520836, 0.10432943, 0.00032552084, 0.17073567, -0.022744497, -0.095479332, 0.009765625, 0.020507812, -0.073893227, 0.093424477, -0.1188151, -0.0096028643, 0.22173564, 0.032389324, 0.043050129, -0.24283855, 0.0859375, -0.13704427, -0.093424477, -0.11979166, 0.04284668, 0.10760498, -0.0013020834, 0.044270832, 0.01953125, -0.13232422, -0.040364582, 0.026428223, 0.16536458, -0.10660807, 0.0390625, -0.050740559, -0.11149088, -0.18977864, -0.088053383, -0.18945312, -0.05826823, -0.065755211, 0.13574219, 0.11783854, -0.030436197, -0.13899739, 0.18701172, -0.089518227, -0.092447914, -0.060791016, 0.16764323, -0.16536458, -0.21777344, 0.21354167, 0.028483072, -0.11962891, -0.095703125, -0.28385416, 0.043111164, 0.25667319, 0.057942707, -0.079223633, -0.28011069, -0.01171875, -0.22591145, -0.29882812, -0.0052083335, -0.12060547, -0.044596355, -0.028605143, -0.10904948, 0.065389, 0.006184896, -0.014160156, 0.009765625, 0.27018228, -0.12662761, 0.1829427, -0.12630208, -0.06363932, -0.19954427, -0.05135091, 0.082519531, 0.023763021, 0.20898438, -0.13118489, -0.24772136, 0.063802086, -0.085449219, 0.31087241, 0.15104167, -0.18196614, -0.13085938, -0.062093098, -0.22591145, 0.030721029, 0.054606121, -0.050618488, 0.01180013, -0.079116821, 0.12141927, -0.21940105, 0.069580078, -0.024902344, -0.047688801, 0.16210938, -0.077392578, -0.234375, 0.11319987, 0.18587239, -0.062337238, 0.0046386719, -0.0029296875, -0.15657552, 0.14811198, -0.20182292, 0.041585285, 0.076009117, -0.19042969, -0.020182291, 0.14583333, -0.18619792, -0.19026692, 0.039876301, -0.00390625, 0.093221031, 0.059407551, 0.034952801, 0.062552132, -0.17252605, -0.023763021, -0.21289062, 0.024291992, -0.20605469, 0.097981773, 0.11360677, 0.031738281, 0.13234894, -0.11598179, 0.12748419, 0.36361602, 0.0088431742, -0.33660072, -0.21929002, -0.11306871, 0.051102459, -0.2234444, 0.22233498, 0.023372233, 0.10255402, -0.14710605, 0.081819378, -0.085164033, 0.24204561, 0.21264954, -0.2792486, -0.24552673, -0.42103291, -0.013051631, 0.064802855, 0.0079477448, -0.2371535, -0.27730566, -0.33468184, 0.18861981, 0.25718945, 0.081765391, -0.029207584, -0.097130194, -0.014269751, 0.10726924, 0.067787945, -0.026175259, -0.1688309, -0.19826111, -0.25108224, 0.0026884563, -0.37198114, 0.37611091, 0.45013016, -0.41105253, -0.12543565, -0.14759138, 0.24954638, -0.11231416, -0.099328972, 0.042546786, -0.0080842786, -0.18369715, 0.063900813, 0.17535141, 0.3612442, 0.056095105, 0.043962762, 0.48170048, 0.024306942, -0.20935604, -0.15560567, -0.073378697, -0.014214471, 0.46347183, -0.12409789, -0.057763524, -0.12993245, -0.047278747, 0.028960925, -0.24761555, 0.11459158, -0.20512313, -0.05654408, 0.032580189, -0.068684183, -0.035274856, -0.22101635, -0.17624252, 0.032608811, 0.29146886, 0.018768199, -0.25392938, -0.048741668, -0.059599452, -0.085385665, 0.11816926, 0.19239232, 0.074761033, 0.010305408, -0.050843764, 0.29070419, 0.05347544, 0.077002995, 0.04925362, -0.35992718, 0.0098826811, -0.041599423, 0.14455026, 0.009395089, -0.02903345, 0.18455216, 0.22396573, -0.043471619, -0.3115963, 0.096926697, -0.056782857, 0.29483032, 0.0058779325, -0.16511288, 0.17191559, 0.01339397, 0.24263257, -0.067996249, 0.29592836, -0.19430877, 0.028600032, 0.23859946, 0.099793896, 0.17568731, 0.019775376, 0.20347743, 0.17322192, -0.037483446, 0.20036781, -0.36326125, -0.10809329, 0.046071861, 0.18584459, 0.17985359, 0.080473423, -0.172703, -0.2632446, -0.10956005, 0.26995397, 0.030537974, 0.093973085, 0.00071671046, -0.25119135, 0.1312242, -0.42486018, 0.059880387, -0.045452792, -0.18427418, 0.16252935, -0.029534444, 0.064462975, -0.10929199, 0.31387481, -0.25519636, -0.49150339, -0.35662395, -0.10631333, 0.14316365, 0.0082015488, 0.022537101, -0.15172559, -0.32533383, -0.25370309, 0.045274679, 0.10513747, 0.29560852, -0.12853448, 0.09074688, -0.33789584, 0.0123588, -0.01621367, 0.32074252, 0.030443193, 0.01046027, 0.08424215, 0.15851986, 0.17397209, 0.10828056, 0.089539468, 0.15792507, 0.25434044, 0.084604658, 0.51793522, -0.18740074, 0.11243076, -0.040148124, 0.12861416, -0.16426955, 0.16630536, 0.27446973, -0.19099113, 0.076746084, -0.055012286, 0.18181381, -0.16199201, -0.23103851, 0.19521853, -0.19725597, -0.032544769, -0.30412146, 0.25892386, 0.02019082, -0.10380098, -0.15367106, 0.053610507, -0.12045395, -0.00056773052, 0.16378757, -0.12003724, -0.012142314, -0.27853298, -0.31371173, 0.22109899, -0.24322721, -0.025665386, -0.030774137, -0.2146576, -0.28979927, 0.21053532, 0.03258035, -0.11805826, -0.069737621, 0.13323413, -0.1270321, 0.12636021, -0.0068928599, -0.032366462, 0.1477164, 0.10592623, 0.02539736, 0.019100977, -0.16992341, -0.0078306086, 0.050490826, 0.092715137, 0.040807061, 0.14163101, -0.027541142, 0.15047002, -0.0085868482, 0.43749359, 0.059279926, 0.017652471, -0.23036662, -0.055822443, 0.17233935, -0.34249955, -0.15532738, 0.18273854, -0.074258216, 0.058435991, 0.30662775, 0.062204786, -0.021404047, -0.15263979, -0.10398483, 0.018704314, 0.098754935, 0.19355863, -0.23659356, 0.26692188, 0.028606759, -0.23384798, -0.061696365, -0.067755416, 0.34500313, -0.10500623, -0.06719739, 0.22870317, -0.10017929, -0.28093302, -0.12707654, 0.15445212, 0.20511679, 0.12344702, 0.14053293, 0.18527097, -0.23536253, 0.056549039, -0.02766126, -0.079165138, -0.31776029, -0.29488158, 0.06560687, 0.021844219, 0.1279849, -0.24778005, -0.032912046, -0.10146494, -0.31063128, 0.026185691, 0.10025069, -0.017481662, 0.0044749379, 0.10383594, -0.18372405, 0.026685279, 0.26957107, -0.11629304, -0.2029227, 0.33337703, 0.22362757, -0.00082864054, -0.11821833, -0.40859896, 0.0084551275, 0.30239031, 0.31064644, 0.096807189, -0.058295999, 0.10336867, 0.1674474, -0.24622965, 0.062542796, 0.10033146, -0.094487563, -0.016273817, 0.10250746, -0.014695862, -0.19672269, -0.01220325, -0.057647407, -0.056649357, 0.19158958, -0.15663612, 0.10762387, -0.21885322, -0.030199345, -0.031964622, 0.090534568, 0.092149131, 0.036358796, -0.092502698, 0.65232861, 0.12958217, 0.060744479, 0.079611674, 0.13678379, -0.07907486, -0.033377782, -0.074413784, -0.10337815, 0.11809172, -0.015936177, 0.36149627, -0.36866188, 0.28348091, -0.043880068, -0.27946579, 0.10290037, -0.12724958, 0.18049918, 0.08478979, -0.074069276, -0.046036199, 0.018442318, 0.026337476, 0.16771992, -0.29805744, -0.027621776, 0.13631475, 0.19907305, -0.044942774, -0.27544114, -0.012878582, -0.080607682, 0.039381102, -0.11516905, -0.099302024, 0.62337887, -0.013073378, -0.12686762, -0.091404207, -0.015386494, -0.011110347, -0.12267289, -0.012666404, 0.013716802, -0.068693861, -0.14216131, 0.040425885, 0.023346113, -0.16910732, 0.087213799, -0.096810408, 0.06854941, 0.27870399, 0.23441166, 0.18948482, -0.13797604, -0.087323681, -0.029124439, -0.23140299, -0.03137207, -0.011078666, -0.00033208448, 0.081115872, -0.10376105, 0.029943723, 0.11510578, 0.11829198, 0.18692496, 0.17119829, 0.13221589, -0.032690126, 0.25623750000000001, 0.16383249999999999, 0.26587892499999999, -0.29904750000000002, -0.052162499999999994, 0.1633735, 0.6732499999999999, -0.0033379999999999972, -0.30065749999999997, -0.029067500000000003, 0.2001, -0.019024999999999993, -0.48361750000000003, 0.1316455, 0.11066450000000001, 0.34009, -0.072419750000000005, 0.2019425, -0.61682999999999999, 0.29904750000000002, 0.19378900000000002, -0.13455499999999998, 0.17202049999999997, 0.36512500000000003, -0.083344999999999989, 0.7332574999999999, -0.31415300000000002, 0.15488550000000001, 0.036452499999999985, -0.1615075, -0.45494000000000001, -0.049581999999999994, -0.5352325, 0.077517249999999996, -0.16386000000000001, -0.26537250000000001, 0.19809750000000001, -0.62273050000000008, 0.16903750000000003, 0.343005, 0.55859000000000003, -0.010168750000000001, 0.042426249999999999, -0.52997874999999994, -0.1668925, 0.33205499999999999, 0.060742500000000005, 0.1916525, 0.19644, 0.43044250000000001, 0.34684999999999999, -0.17114499999999999, 0.044275000000000009, -0.00090249999999998665, 0.38488499999999998, 0.014217000000000004, -0.29512699999999997, -0.14120450000000001, -0.188171, 0.48175000000000001, 0.20212249999999998, 0.29997525000000003, -0.53519775000000003, -0.00099975000000000411, 0.21142624999999998, 0.230435, 0.40160125000000002, -0.10604749999999999, -0.14359, -0.08079749999999998, 0.54250750000000003, -0.0038320000000000021, 0.036580999999999995, 0.22300249999999999, 0.33543149999999999, -0.019860000000000017, -0.213224, 0.099517999999999968, 0.25498999999999999, -0.064345750000000007, 0.052218500000000001, 0.59423250000000005, 0.19102574999999999, -0.31345525000000002, 0.16077824999999998, 0.056635500000000005, -0.065812499999999996, -0.067752000000000007, -0.21012, -0.040337500000000005, -0.42603276000000001, -0.76480449999999989, 0.29216999999999999, 0.54973249999999996, -0.64556250000000004, 0.059822500000000015, 0.10337500000000001, 0.30703249999999999, 0.34249250000000003, -0.31181999999999999, 0.10007149999999998, -0.070112500000000022, -0.11803424999999999, 0.10523974999999999, -0.081493999999999997, 0.0173175, -0.23730749999999998, 0.051147499999999992, 0.087402999999999995, -0.12373750000000003, 0.056807500000000025, 0.24101600000000001, -0.28299774999999994, -0.31963477500000004, -0.2142, 0.46469249999999995, 0.079819749999999995, 0.0019303049250000001, 0.230379, 0.0054725000000000017, -0.29068499999999997, 0.34981499999999999, -0.02224750000000001, -0.13200499999999998, -0.29048650000000004, 0.0091149999999999773, -0.40205125000000003, -0.35346750000000005, 0.52177000000000007, -0.50439250000000002, 0.14634915000000001, 0.36091249999999997, -0.088252500000000011, -0.11804024999999999, 0.080297499999999994, -0.37862125000000002, -0.51719749999999998, -0.14251075000000002, 0.040730000000000002, 0.10149499999999999, 0.27017249999999998, 0.18704500000000002, -0.15373249999999999, -0.17414249999999998, -0.1508478, -0.0049524999999999916, -0.13695237499999999, -0.026329500000000006, -0.49820674999999998, -0.28463300000000002, 0.17477375000000001, 0.031885000000000011, -3.3622000000000001, 0.16580699999999998, -0.240759, -0.49330750000000001, -0.32047024999999996, -0.17989999999999998, 0.24474500000000002, -0.14516024999999999, 0.19675000000000001, 0.088478000000000001, 0.048742499999999966, 0.0024924999999999947, 0.25057374999999998, -0.17991000000000001, -0.014982249999999999, -0.05766000000000001, 0.37269749999999996, -0.11185249999999999, -0.01200749999999999, 0.1007605, 0.23689075000000004, -0.10776924999999998, -0.27692499999999998, -0.23103225000000002, -0.17133000000000004, -0.18601000000000001, -0.21628249999999999, -0.11182750000000002, 0.14447950000000001, 0.48393500000000006, 0.026182, 0.032756600000000004, 0.095299499999999995, -0.31934249999999997, 0.10525749999999999, 0.185305, 0.072421249999999993, -0.33165500000000003, -0.10606500000000002, -0.26895249999999998, 0.10577575000000002, -0.46697, 0.058436249999999995, 0.34360749999999995, -0.42619499999999999, 0.53484749999999992, -0.58440650000000005, 0.2953925, 1.0, 3.7350000000000003, 13.950225000000003, 52.10409037500002, 194.6087775506251, 726.8637841515848, 2714.8362338061693, 10139.913333266044, 37872.576299748675, 141454.07247956132, 528330.9607111616, 1.0, 0.0, 0.75, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.421875, 0.0, 0.0, 0.0, 0.0, 0.31640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2373046875, 1.0, 1.0, -1.1012499999999998, 44579.75, 31772.75, 1.2127515624999996, -49093.44968749999, -34989.74093749999, 1987354110.0625, 1416421251.8125, 1009507642.5625, -1.3355426582031245, 54064.16146835936, 38532.45220742186, -2188573713.7063274, -1559833903.5585153, -1111720291.3719528, 88595749388058.73, 63143705300488.3, 45003598328525.61, 32074833950227.67, 1.4707663523461907, -59538.15781703074, -42433.862993423325, 2410166802.2190933, 1717767086.293815, 1224281970.873363, -97566069013599.66, -69537005462162.72, -49560212659288.82, -35322410887688.21, 3.9495763587823114e+18, 2.8149305963694433e+18, 2.0062491625860895e+18, 1.429888078792662e+18, 1.0191056803920963e+18, -1.6196814455212423, 65566.3962960051, 46730.29162150743, -2654196190.943776, -1891691003.7810636, -1348240520.424291, 107444633501226.62, 76577627265206.7, 54578184191041.805, 38898804990066.65, -4.349470965109019e+18, -3.0999423192518487e+18, -2.2093818902979305e+18, -1.574664246770419e+18, -1.1222901305317956e+18, 1.7607112668042576e+23, 1.2548890225350068e+23, 8.943808610579723e+22, 6.374405308055718e+22, 4.543147645545955e+22, 3.2379790006677976e+22, 1.0, -1.2366666666666666, 42.333333333333336, 331.0, 1.5293444444444442, -52.352222222222224, -409.33666666666664, 1792.1111111111113, 14012.333333333334, 109561.0, -1.8912892962962957, 64.74224814814814, 506.21301111111103, -2216.2440740740744, -17328.585555555557, -135490.43666666665, 75866.03703703705, 593188.7777777779, 4638082.333333334, 36264691.0, 2.3388944297530854, -80.06458020987652, -626.0167570740739, 2740.755171604938, 21429.68413703703, 167556.50667777774, -93820.99913580249, -733576.7885185187, -5735761.81888889, -44847334.53666666, 3211662.234567902, 25111658.259259265, 196345485.44444448, 1535205252.3333335, 12003612721.0, -2.892432778127982, 99.01319752621396, 774.1740562482713, -3389.400562218106, -26501.37604946913, -207211.54659151845, 116025.30226460904, 907189.9618012344, 7093225.449359258, 55461203.710344434, -3971755.6300823055, -31054750.713950623, -242813916.99962968, -1898537162.0522225, -14844467731.636665, 135960367.9300412, 1063060199.6419755, 8311958883.814816, 64990355682.11112, 508152938522.3334, 3973195810651.0, 1.0, -4.999, 0.6666666666666666, 6.833333333333333, 24.990000999999996, -3.3326666666666664, -34.15983333333333, 0.4444444444444444, 4.555555555555555, 46.69444444444444, -124.92501499899997, 16.66000066666666, 170.7650068333333, -2.2217777777777776, -22.77322222222222, -233.42552777777775, 0.2962962962962963, 3.0370370370370368, 31.129629629629626, 319.0787037037037, 624.5001499800009, -83.28334333266665, -853.6542691598331, 11.106667111111108, 113.84333788888885, 1166.8942133611108, -1.481185185185185, -15.182148148148146, -155.6170185185185, -1595.0744398148145, 0.19753086419753085, 2.024691358024691, 20.753086419753085, 212.7191358024691, 2180.3711419753085, -3121.8762497500243, 416.33343332000055, 4267.417691530006, -55.52222888844443, -569.1028461065554, -5833.304172592193, 7.404444740740738, 75.89555859259256, 777.9294755740738, 7973.777124634257, -0.9874567901234566, -10.12143209876543, -103.74467901234566, -1063.382959876543, -10899.675338734565, 0.1316872427983539, 1.3497942386831274, 13.835390946502054, 141.81275720164606, 1453.5807613168722, 14899.202803497941, 1.0, -1.1012499999999998, 44579.75, 31772.75, 1.2127515624999996, -49093.44968749999, -34989.74093749999, 1987354110.0625, 1416421251.8125, 1009507642.5625, -1.3355426582031245, 54064.16146835936, 38532.45220742186, -2188573713.7063274, -1559833903.5585153, -1111720291.3719528, 88595749388058.73, 63143705300488.3, 45003598328525.61, 32074833950227.67, 1.4707663523461907, -59538.15781703074, -42433.862993423325, 2410166802.2190933, 1717767086.293815, 1224281970.873363, -97566069013599.66, -69537005462162.72, -49560212659288.82, -35322410887688.21, 3.9495763587823114e+18, 2.8149305963694433e+18, 2.0062491625860895e+18, 1.429888078792662e+18, 1.0191056803920963e+18, -1.6196814455212423, 65566.3962960051, 46730.29162150743, -2654196190.943776, -1891691003.7810636, -1348240520.424291, 107444633501226.62, 76577627265206.7, 54578184191041.805, 38898804990066.65, -4.349470965109019e+18, -3.0999423192518487e+18, -2.2093818902979305e+18, -1.574664246770419e+18, -1.1222901305317956e+18, 1.7607112668042576e+23, 1.2548890225350068e+23, 8.943808610579723e+22, 6.374405308055718e+22, 4.543147645545955e+22, 3.2379790006677976e+22, 1.0, -1.2366666666666666, 42.333333333333336, 331.0, 1.5293444444444442, -52.352222222222224, -409.33666666666664, 1792.1111111111113, 14012.333333333334, 109561.0, -1.8912892962962957, 64.74224814814814, 506.21301111111103, -2216.2440740740744, -17328.585555555557, -135490.43666666665, 75866.03703703705, 593188.7777777779, 4638082.333333334, 36264691.0, 2.3388944297530854, -80.06458020987652, -626.0167570740739, 2740.755171604938, 21429.68413703703, 167556.50667777774, -93820.99913580249, -733576.7885185187, -5735761.81888889, -44847334.53666666, 3211662.234567902, 25111658.259259265, 196345485.44444448, 1535205252.3333335, 12003612721.0, -2.892432778127982, 99.01319752621396, 774.1740562482713, -3389.400562218106, -26501.37604946913, -207211.54659151845, 116025.30226460904, 907189.9618012344, 7093225.449359258, 55461203.710344434, -3971755.6300823055, -31054750.713950623, -242813916.99962968, -1898537162.0522225, -14844467731.636665, 135960367.9300412, 1063060199.6419755, 8311958883.814816, 64990355682.11112, 508152938522.3334, 3973195810651.0, 1.0, 10.67471845028449, 113.94961399284409, 1216.3800468922084, 12984.514529118169, 138606.03681196473, 1479580.418477491, 15794104.391801318, 168597617.5568808, 1799732098.8084435, 19211633440.71972, 1.0, -0.521, 18293.5, 24140.25, 0.27144100000000004, -9530.9135, -12577.07025, 334652142.25, 441609663.375, 582751670.0625, -0.14142076100000003, 4965.605933500001, 6552.653600250001, -174353766.11225, -230078634.618375, -303613620.1025625, 6121958964250.375, 8078586376950.5625, 10660567676288.344, 14067771003226.266, 0.07368021648100002, -2587.0806913535007, -3413.9325257302507, 90838312.14448227, 119870968.6361734, 158182696.07343507, -3189540620374.4453, -4208943502391.243, -5554155759346.228, -7329308692680.884, 1.1199205631251424e+17, 1.4778561988674512e+17, 1.9501909478618083e+17, 2.5734876884751968e+17, 3.395995089606328e+17, -0.03838739278660101, 1347.869040195174, 1778.6588459054608, -47326760.627275266, -62452774.659446344, -82413184.65425968, 1661750663215.0864, 2192859564745.8381, 2893715150619.385, 3818569828886.7407, -5.834786133881991e+16, -7.69963079609942e+16, -1.0160494838360021e+17, -1.3407870856955778e+17, -1.769313441684897e+17, 2.0487266821529791e+21, 2.7035162373981717e+21, 3.567581810470999e+21, 4.7078097029121017e+21, 6.212463617171337e+21, 8.198017046186917e+21, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -4.999, 2.0, 16.0, 24.990000999999996, -9.998, -79.984, 4.0, 32.0, 256.0, -124.92501499899997, 49.98000199999999, 399.84001599999993, -19.996, -159.968, -1279.744, 8.0, 64.0, 512.0, 4096.0, 624.5001499800009, -249.85002999799994, -1998.8002399839995, 99.96000399999998, 799.6800319999999, 6397.440255999999, -39.992, -319.936, -2559.488, -20475.904, 16.0, 128.0, 1024.0, 8192.0, 65536.0, -3121.8762497500243, 1249.0002999600017, 9992.002399680014, -499.7000599959999, -3997.600479967999, -31980.803839743992, 199.92000799999997, 1599.3600639999997, 12794.880511999998, 102359.04409599998, -79.984, -639.872, -5118.976, -40951.808, -327614.464, 32.0, 256.0, 2048.0, 16384.0, 131072.0, 1048576.0, 1.0, -1.5627499999999999, 9.75, 85.0, 2.4421875624999996, -15.2368125, -132.83374999999998, 95.0625, 828.75, 7225.0, -3.816528613296874, 23.811328734374996, 207.58594281249995, -148.55892187499998, -1295.1290625, -11290.868749999998, 926.859375, 8080.3125, 70443.75, 614125.0, 5.964280090429689, -37.21115397964452, -324.4049321302343, 232.1604551601562, 2023.9629424218747, 17644.805139062497, -1448.4494882812498, -12627.508359374999, -110085.9703125, -959723.8437499998, 9036.87890625, 78783.046875, 686826.5625, 5987718.75, 52200625.0, -9.320678711318996, 58.15173088168947, 506.9638076865236, -362.8087513015341, -3162.9480882697844, -27574.419231069915, 2263.564437811523, 19733.63868861328, 172036.85010585934, 1499808.4368203122, -14122.382510742185, -123118.20650390624, -1073338.210546875, -9357307.4765625, -81576526.71874999, 88109.5693359375, 768134.70703125, 6696558.984375, 58380257.8125, 508956093.75, 4437053125.0, 1.0, -1.322, 11.0, 57.0, 1.7476840000000002, -14.542000000000002, -75.354, 121.0, 627.0, 3249.0, -2.3104382480000005, 19.224524000000002, 99.61798800000001, -159.96200000000002, -828.8940000000001, -4295.178, 1331.0, 6897.0, 35739.0, 185193.0, 3.054399363856001, -25.414820728000006, -131.69498013600003, 211.46976400000003, 1095.797868, 5678.225316000001, -1759.582, -9117.834, -47246.958000000006, -244825.146, 14641.0, 75867.0, 393129.0, 2037123.0, 10556001.0, -4.037915959017633, 33.59839300241601, 174.10076373979206, -279.56302800800006, -1448.6447814960004, -7506.613867752001, 2326.1674040000003, 12053.776548000002, 62460.478476000004, 323658.843012, -19355.402000000002, -100296.174, -519716.53800000006, -2693076.606, -13955033.322, 161051.0, 834537.0, 4324419.0, 22408353.0, 116116011.0, 601692057.0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing tweet 0\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "test_tweets = read_test_data(test_data_file_path)\n",
    "with open(test_data_file_path + \".cleaned\", 'w') as cleaned_input_file:\n",
    "    for tweet in test_tweets:\n",
    "        cleaned_input_file.write(tweet.id + \"\\t\" + tweet.text + \"\\n\")\n",
    "\n",
    "tweet_test = list()\n",
    "for tweet in test_tweets:\n",
    "    tweet_test.append(tweet.text)\n",
    "\n",
    "x_test = vectorize_tweets(tweet_test)\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### sadness\n",
      "| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\n",
      "| --- | --- | --- | --- |\n",
      "| 0.665429178387 | 0.657671226336 | 0.433362835507 | 0.403499767499 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble, svm, model_selection\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "ml_model = ensemble.GradientBoostingRegressor(max_depth=3, n_estimators=100)\n",
    "# ml_model = ensemble.AdaBoostRegressor()\n",
    "# ml_model = XGBRegressor(max_depth=1, n_estimators=100)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "score_train = np.array(score_train)\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores = np.zeros(4)\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_test = score_train[train_index], score_train[test_index]\n",
    "    ml_model.fit(X_train, y_train)\n",
    "    y_pred = ml_model.predict(X_test)\n",
    "    scores += evaluate_lists(y_pred, y_test)\n",
    "\n",
    "avg_scores = scores/5\n",
    "print(\"### \" + emotion)\n",
    "print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "print(\"| --- | --- | --- | --- |\")\n",
    "print(\"| \" + str(avg_scores[0]) + \" | \" + str(avg_scores[1]) + \" | \" + \\\n",
    "      str(avg_scores[2]) + \" | \" + str(avg_scores[3]) + \" |\")\n",
    "\n",
    "ml_model.fit(x_train, score_train)\n",
    "y_test = ml_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gold_tweets = read_training_data(gold_set_path)\n",
    "\n",
    "y_gold = list()\n",
    "data_dict = dict()\n",
    "diff = 0\n",
    "for i in range(len(y_gold_tweets)):\n",
    "    y_gold.append(y_gold_tweets[i].intensity)\n",
    "    if y_gold_tweets[i].intensity >= 0.5:\n",
    "        diff += y_gold_tweets[i].intensity - y_test[i]\n",
    "#         print([tweet_test[i], str(y_test[i]), str(y_gold[i].intensity)])\n",
    "# print(diff/len(y_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(predictions_file_path, 'w') as predictions_file:\n",
    "    for i in range(len(y_test)):\n",
    "        predictions_file.write(\n",
    "            str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "            test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation based on Pearson and Spearman co-efficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### sadness\n",
      "| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\n",
      "| --- | --- | --- | --- |\n",
      "| 0.55052886127 | 0.593330566191 | 0.433289290458 | 0.367981815824 |\n"
     ]
    }
   ],
   "source": [
    "print(\"### \" + emotion)\n",
    "print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "print(\"| --- | --- | --- | --- |\")\n",
    "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1 = \\\n",
    "    evaluate(predictions_file_path, gold_set_path)\n",
    "print(\"| \" + str(pears_corr) + \" | \" + str(spear_corr) + \" | \" + \\\n",
    "      str(pears_corr_range_05_1) + \" | \" + str(spear_corr_range_05_1) + \" |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overall Score Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pears_corr_sum = 0\n",
    "spear_corr_sum = 0\n",
    "pears_corr_range_05_1_sum = 0\n",
    "spear_corr_range_05_1_sum = 0\n",
    "\n",
    "for emotion in ['anger', 'fear', 'sadness', 'joy']:\n",
    "    print(\"\\n### \" + emotion)\n",
    "    predictions_file_path = \\\n",
    "        wassa_home + \"predictions/\" + \\\n",
    "        emotion + \"-pred.txt\"\n",
    "    gold_set_path = \\\n",
    "        wassa_home + \"dataset/gold-set/\" + \\\n",
    "        emotion + \"-ratings-0to1.dev.gold.txt\"\n",
    "    print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "    print(\"| --- | --- | --- | --- |\")\n",
    "    pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1 = \\\n",
    "        evaluate(predictions_file_path, gold_set_path)\n",
    "    print(\"| \" + str(pears_corr) + \" | \" + str(spear_corr) + \" | \" + \\\n",
    "          str(pears_corr_range_05_1) + \" | \" + str(spear_corr_range_05_1) + \" |\")\n",
    "    pears_corr_sum += pears_corr\n",
    "    spear_corr_sum += spear_corr\n",
    "    pears_corr_range_05_1_sum += pears_corr_range_05_1\n",
    "    spear_corr_range_05_1_sum += spear_corr_range_05_1\n",
    "    \n",
    "print(\"\\n### Average Scores\")\n",
    "print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "print(\"| --- | --- | --- | --- |\")\n",
    "print(\"| \" + str(pears_corr_sum/4) + \" | \" + str(spear_corr_sum/4) + \" | \" + \\\n",
    "      str(pears_corr_range_05_1_sum/4) + \" | \" + str(spear_corr_range_05_1_sum/4) + \" |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "\n",
    "\n",
    "def vectorize_tweets(tweet_list, bin_string, vector_dict):\n",
    "    \n",
    "    vectors = list()\n",
    "\n",
    "    for i in range(len(tweet_list)):        \n",
    "        x_vector = list()\n",
    "        \n",
    "        '''Pre-trained Word embeddings'''\n",
    "        index = 0\n",
    "        if is_active_vector_method(bin_string[index]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_word2vec_embedding(tweet_list[i], wv_model, w2v_dimensions)\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "        \n",
    "        index = 1\n",
    "        if is_active_vector_method(bin_string[index]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_word2vec_embedding(tweet_list[i], wv_model_1, w2v_dimensions_1)\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "            \n",
    "        index = 2\n",
    "        if is_active_vector_method(bin_string[index]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_word2vec_embedding(tweet_list[i], wv_model_2, w2v_dimensions_2)\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "\n",
    "        '''NRC Emotion Intensity Lexicon'''\n",
    "        index = 3\n",
    "        if is_active_vector_method(bin_string[index]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_emo_int_vector(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "\n",
    "        '''WordNet'''\n",
    "        index = 4\n",
    "        if is_active_vector_method(bin_string[index]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_sentiwordnetscore(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "\n",
    "        '''NRC Sentiment Lexica'''\n",
    "        index = 5\n",
    "        if is_active_vector_method(bin_string[5]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_sentiment_emotion_feature(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "            \n",
    "        index = 6\n",
    "        if is_active_vector_method(bin_string[6]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_sentiment_emoticon_lexicon_vector(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "            \n",
    "        index = 7\n",
    "        if is_active_vector_method(bin_string[7]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_sentiment_emoticon_afflex_vector(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "\n",
    "        '''NRC Hashtag Lexica'''\n",
    "        index = 8\n",
    "        if is_active_vector_method(bin_string[8]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_hashtag_emotion_vector(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "            \n",
    "        index = 9\n",
    "        if is_active_vector_method(bin_string[9]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_sentiment_hash_sent_lex_vector(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "            \n",
    "        index = 10\n",
    "        if is_active_vector_method(bin_string[10]):\n",
    "            if str(index) + \"-\" + str(i) not in vector_dict.keys():\n",
    "                tmp_vector = get_sentiment_hashtag_affneglex_vector(tweet_list[i])\n",
    "                vector_dict[str(index) + \"-\" + str(i)] = tmp_vector\n",
    "            x_vector.extend(vector_dict[str(index) + \"-\" + str(i)])\n",
    "\n",
    "        vectors.append(x_vector)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(x_train, score_train, x_test, y_gold):\n",
    "    ml_model = ensemble.GradientBoostingRegressor(max_depth=3, n_estimators=100)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    score_train = np.array(score_train)\n",
    "    num_splits = 10\n",
    "\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True)\n",
    "\n",
    "    scores = np.zeros(4)\n",
    "    for train_index, test_index in kf.split(x_train):\n",
    "        X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "        y_train, y_test = score_train[train_index], score_train[test_index]\n",
    "        ml_model.fit(X_train, y_train)\n",
    "        y_pred = ml_model.predict(X_test)\n",
    "        scores += evaluate_lists(y_pred, y_test)\n",
    "    train_scores = scores/num_splits\n",
    "\n",
    "    ml_model.fit(x_train, score_train)\n",
    "    y_test = ml_model.predict(x_test)\n",
    "    \n",
    "    test_scores = evaluate_lists(y_test, y_gold)\n",
    "    \n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all_data(emotion):\n",
    "    training_data_file_path = \\\n",
    "        wassa_home + \"dataset/\" + \\\n",
    "        emotion + \"-ratings-0to1.train.txt\"\n",
    "    test_data_file_path = \\\n",
    "        wassa_home + \"dataset/\" + \\\n",
    "        emotion + \"-ratings-0to1.dev.target.txt\"\n",
    "    predictions_file_path = \\\n",
    "        wassa_home + \"predictions/\" + \\\n",
    "        emotion + \"-pred.txt\"\n",
    "    gold_set_path = \\\n",
    "        wassa_home + \"dataset/gold-set/\" + \\\n",
    "        emotion + \"-ratings-0to1.dev.gold.txt\"\n",
    "        \n",
    "    training_tweets = read_training_data(training_data_file_path)\n",
    "\n",
    "    score_train = list()\n",
    "    tweet_train = list()\n",
    "    for tweet in training_tweets:\n",
    "        tweet_train.append(tweet.text)\n",
    "        score_train.append(float(tweet.intensity))\n",
    "        \n",
    "    test_tweets = read_test_data(test_data_file_path)\n",
    "    tweet_test = list()\n",
    "    for tweet in test_tweets:\n",
    "        tweet_test.append(tweet.text)\n",
    "        \n",
    "    gold_tweets = read_training_data(gold_set_path)\n",
    "    y_gold = list()\n",
    "    for tweet in gold_tweets:\n",
    "        y_gold.append(tweet.intensity)\n",
    "        \n",
    "    return tweet_train, tweet_test, score_train, y_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: sadness\n",
      "Current test: 1/2048\n",
      "Current test: 2/2048\n",
      "Current test: 3/2048\n",
      "Current test: 4/2048\n",
      "Current test: 5/2048\n",
      "Current test: 6/2048\n",
      "Current test: 7/2048\n",
      "Current test: 8/2048\n",
      "Current test: 9/2048\n",
      "Current test: 10/2048\n",
      "Current test: 11/2048\n",
      "Current test: 12/2048\n",
      "Current test: 13/2048\n",
      "Current test: 14/2048\n",
      "Current test: 15/2048\n",
      "Current test: 16/2048\n",
      "Current test: 17/2048\n",
      "Current test: 18/2048\n",
      "Current test: 19/2048\n",
      "Current test: 20/2048\n",
      "Current test: 21/2048\n",
      "Current test: 22/2048\n",
      "Current test: 23/2048\n",
      "Current test: 24/2048\n",
      "Current test: 25/2048\n",
      "Current test: 26/2048\n",
      "Current test: 27/2048\n",
      "Current test: 28/2048\n",
      "Current test: 29/2048\n",
      "Current test: 30/2048\n",
      "Current test: 31/2048\n",
      "Current test: 32/2048\n",
      "Current test: 33/2048\n",
      "Current test: 34/2048\n",
      "Current test: 35/2048\n",
      "Current test: 36/2048\n",
      "Current test: 37/2048\n",
      "Current test: 38/2048\n",
      "Current test: 39/2048\n",
      "Current test: 40/2048\n",
      "Current test: 41/2048\n",
      "Current test: 42/2048\n",
      "Current test: 43/2048\n",
      "Current test: 44/2048\n",
      "Current test: 45/2048\n",
      "Current test: 46/2048\n",
      "Current test: 47/2048\n",
      "Current test: 48/2048\n",
      "Current test: 49/2048\n",
      "Current test: 50/2048\n",
      "Current test: 51/2048\n",
      "Current test: 52/2048\n",
      "Current test: 53/2048\n",
      "Current test: 54/2048\n",
      "Current test: 55/2048\n",
      "Current test: 56/2048\n",
      "Current test: 57/2048\n",
      "Current test: 58/2048\n",
      "Current test: 59/2048\n",
      "Current test: 60/2048\n",
      "Current test: 61/2048\n",
      "Current test: 62/2048\n",
      "Current test: 63/2048\n",
      "Current test: 64/2048\n",
      "Current test: 65/2048\n",
      "Current test: 66/2048\n",
      "Current test: 67/2048\n",
      "Current test: 68/2048\n",
      "Current test: 69/2048\n",
      "Current test: 70/2048\n",
      "Current test: 71/2048\n",
      "Current test: 72/2048\n",
      "Current test: 73/2048\n",
      "Current test: 74/2048\n",
      "Current test: 75/2048\n",
      "Current test: 76/2048\n",
      "Current test: 77/2048\n",
      "Current test: 78/2048\n",
      "Current test: 79/2048\n",
      "Current test: 80/2048\n",
      "Current test: 81/2048\n",
      "Current test: 82/2048\n",
      "Current test: 83/2048\n",
      "Current test: 84/2048\n",
      "Current test: 85/2048\n",
      "Current test: 86/2048\n",
      "Current test: 87/2048\n",
      "Current test: 88/2048\n",
      "Current test: 89/2048\n",
      "Current test: 90/2048\n",
      "Current test: 91/2048\n",
      "Current test: 92/2048\n",
      "Current test: 93/2048\n",
      "Current test: 94/2048\n",
      "Current test: 95/2048\n",
      "Current test: 96/2048\n",
      "Current test: 97/2048\n",
      "Current test: 98/2048\n",
      "Current test: 99/2048\n",
      "Current test: 100/2048\n",
      "Current test: 101/2048\n",
      "Current test: 102/2048\n",
      "Current test: 103/2048\n",
      "Current test: 104/2048\n",
      "Current test: 105/2048\n",
      "Current test: 106/2048\n",
      "Current test: 107/2048\n",
      "Current test: 108/2048\n",
      "Current test: 109/2048\n",
      "Current test: 110/2048\n",
      "Current test: 111/2048\n",
      "Current test: 112/2048\n",
      "Current test: 113/2048\n",
      "Current test: 114/2048\n",
      "Current test: 115/2048\n",
      "Current test: 116/2048\n",
      "Current test: 117/2048\n",
      "Current test: 118/2048\n",
      "Current test: 119/2048\n",
      "Current test: 120/2048\n",
      "Current test: 121/2048\n",
      "Current test: 122/2048\n",
      "Current test: 123/2048\n",
      "Current test: 124/2048\n",
      "Current test: 125/2048\n",
      "Current test: 126/2048\n",
      "Current test: 127/2048\n",
      "Current test: 128/2048\n",
      "Current test: 129/2048\n",
      "Current test: 130/2048\n",
      "Current test: 131/2048\n",
      "Current test: 132/2048\n",
      "Current test: 133/2048\n",
      "Current test: 134/2048\n",
      "Current test: 135/2048\n",
      "Current test: 136/2048\n",
      "Current test: 137/2048\n",
      "Current test: 138/2048\n",
      "Current test: 139/2048\n",
      "Current test: 140/2048\n",
      "Current test: 141/2048\n",
      "Current test: 142/2048\n",
      "Current test: 143/2048\n",
      "Current test: 144/2048\n",
      "Current test: 145/2048\n",
      "Current test: 146/2048\n",
      "Current test: 147/2048\n",
      "Current test: 148/2048\n",
      "Current test: 149/2048\n",
      "Current test: 150/2048\n",
      "Current test: 151/2048\n",
      "Current test: 152/2048\n",
      "Current test: 153/2048\n",
      "Current test: 154/2048\n",
      "Current test: 155/2048\n",
      "Current test: 156/2048\n",
      "Current test: 157/2048\n",
      "Current test: 158/2048\n",
      "Current test: 159/2048\n",
      "Current test: 160/2048\n",
      "Current test: 161/2048\n",
      "Current test: 162/2048\n",
      "Current test: 163/2048\n",
      "Current test: 164/2048\n",
      "Current test: 165/2048\n",
      "Current test: 166/2048\n",
      "Current test: 167/2048\n",
      "Current test: 168/2048\n",
      "Current test: 169/2048\n",
      "Current test: 170/2048\n",
      "Current test: 171/2048\n",
      "Current test: 172/2048\n",
      "Current test: 173/2048\n",
      "Current test: 174/2048\n",
      "Current test: 175/2048\n",
      "Current test: 176/2048\n",
      "Current test: 177/2048\n",
      "Current test: 178/2048\n",
      "Current test: 179/2048\n",
      "Current test: 180/2048\n",
      "Current test: 181/2048\n",
      "Current test: 182/2048\n",
      "Current test: 183/2048\n",
      "Current test: 184/2048\n",
      "Current test: 185/2048\n",
      "Current test: 186/2048\n",
      "Current test: 187/2048\n",
      "Current test: 188/2048\n",
      "Current test: 189/2048\n",
      "Current test: 190/2048\n",
      "Current test: 191/2048\n",
      "Current test: 192/2048\n",
      "Current test: 193/2048\n",
      "Current test: 194/2048\n",
      "Current test: 195/2048\n",
      "Current test: 196/2048\n",
      "Current test: 197/2048\n",
      "Current test: 198/2048\n",
      "Current test: 199/2048\n",
      "Current test: 200/2048\n",
      "Current test: 201/2048\n",
      "Current test: 202/2048\n",
      "Current test: 203/2048\n",
      "Current test: 204/2048\n",
      "Current test: 205/2048\n",
      "Current test: 206/2048\n",
      "Current test: 207/2048\n",
      "Current test: 208/2048\n",
      "Current test: 209/2048\n",
      "Current test: 210/2048\n",
      "Current test: 211/2048\n",
      "Current test: 212/2048\n",
      "Current test: 213/2048\n",
      "Current test: 214/2048\n",
      "Current test: 215/2048\n",
      "Current test: 216/2048\n",
      "Current test: 217/2048\n",
      "Current test: 218/2048\n",
      "Current test: 219/2048\n",
      "Current test: 220/2048\n",
      "Current test: 221/2048\n",
      "Current test: 222/2048\n",
      "Current test: 223/2048\n",
      "Current test: 224/2048\n",
      "Current test: 225/2048\n",
      "Current test: 226/2048\n",
      "Current test: 227/2048\n",
      "Current test: 228/2048\n",
      "Current test: 229/2048\n",
      "Current test: 230/2048\n",
      "Current test: 231/2048\n",
      "Current test: 232/2048\n",
      "Current test: 233/2048\n",
      "Current test: 234/2048\n",
      "Current test: 235/2048\n",
      "Current test: 236/2048\n",
      "Current test: 237/2048\n",
      "Current test: 238/2048\n",
      "Current test: 239/2048\n",
      "Current test: 240/2048\n",
      "Current test: 241/2048\n",
      "Current test: 242/2048\n",
      "Current test: 243/2048\n",
      "Current test: 244/2048\n",
      "Current test: 245/2048\n",
      "Current test: 246/2048\n",
      "Current test: 247/2048\n",
      "Current test: 248/2048\n",
      "Current test: 249/2048\n",
      "Current test: 250/2048\n",
      "Current test: 251/2048\n",
      "Current test: 252/2048\n",
      "Current test: 253/2048\n",
      "Current test: 254/2048\n",
      "Current test: 255/2048\n",
      "Current test: 256/2048\n",
      "Current test: 257/2048\n",
      "Current test: 258/2048\n",
      "Current test: 259/2048\n",
      "Current test: 260/2048\n",
      "Current test: 261/2048\n",
      "Current test: 262/2048\n",
      "Current test: 263/2048\n",
      "Current test: 264/2048\n",
      "Current test: 265/2048\n",
      "Current test: 266/2048\n",
      "Current test: 267/2048\n",
      "Current test: 268/2048\n",
      "Current test: 269/2048\n",
      "Current test: 270/2048\n",
      "Current test: 271/2048\n",
      "Current test: 272/2048\n",
      "Current test: 273/2048\n",
      "Current test: 274/2048\n",
      "Current test: 275/2048\n",
      "Current test: 276/2048\n",
      "Current test: 277/2048\n",
      "Current test: 278/2048\n",
      "Current test: 279/2048\n",
      "Current test: 280/2048\n",
      "Current test: 281/2048\n",
      "Current test: 282/2048\n",
      "Current test: 283/2048\n",
      "Current test: 284/2048\n",
      "Current test: 285/2048\n",
      "Current test: 286/2048\n",
      "Current test: 287/2048\n",
      "Current test: 288/2048\n",
      "Current test: 289/2048\n",
      "Current test: 290/2048\n",
      "Current test: 291/2048\n",
      "Current test: 292/2048\n",
      "Current test: 293/2048\n",
      "Current test: 294/2048\n",
      "Current test: 295/2048\n",
      "Current test: 296/2048\n",
      "Current test: 297/2048\n",
      "Current test: 298/2048\n",
      "Current test: 299/2048\n",
      "Current test: 300/2048\n",
      "Current test: 301/2048\n",
      "Current test: 302/2048\n",
      "Current test: 303/2048\n",
      "Current test: 304/2048\n",
      "Current test: 305/2048\n",
      "Current test: 306/2048\n",
      "Current test: 307/2048\n",
      "Current test: 308/2048\n",
      "Current test: 309/2048\n",
      "Current test: 310/2048\n",
      "Current test: 311/2048\n",
      "Current test: 312/2048\n",
      "Current test: 313/2048\n",
      "Current test: 314/2048\n",
      "Current test: 315/2048\n",
      "Current test: 316/2048\n",
      "Current test: 317/2048\n",
      "Current test: 318/2048\n",
      "Current test: 319/2048\n",
      "Current test: 320/2048\n",
      "Current test: 321/2048\n",
      "Current test: 322/2048\n",
      "Current test: 323/2048\n",
      "Current test: 324/2048\n",
      "Current test: 325/2048\n",
      "Current test: 326/2048\n",
      "Current test: 327/2048\n",
      "Current test: 328/2048\n",
      "Current test: 329/2048\n",
      "Current test: 330/2048\n",
      "Current test: 331/2048\n",
      "Current test: 332/2048\n",
      "Current test: 333/2048\n",
      "Current test: 334/2048\n",
      "Current test: 335/2048\n",
      "Current test: 336/2048\n",
      "Current test: 337/2048\n",
      "Current test: 338/2048\n",
      "Current test: 339/2048\n",
      "Current test: 340/2048\n",
      "Current test: 341/2048\n",
      "Current test: 342/2048\n",
      "Current test: 343/2048\n",
      "Current test: 344/2048\n",
      "Current test: 345/2048\n",
      "Current test: 346/2048\n",
      "Current test: 347/2048\n",
      "Current test: 348/2048\n",
      "Current test: 349/2048\n",
      "Current test: 350/2048\n",
      "Current test: 351/2048\n",
      "Current test: 352/2048\n",
      "Current test: 353/2048\n",
      "Current test: 354/2048\n",
      "Current test: 355/2048\n",
      "Current test: 356/2048\n",
      "Current test: 357/2048\n",
      "Current test: 358/2048\n",
      "Current test: 359/2048\n",
      "Current test: 360/2048\n",
      "Current test: 361/2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current test: 362/2048\n",
      "Current test: 363/2048\n",
      "Current test: 364/2048\n",
      "Current test: 365/2048\n",
      "Current test: 366/2048\n",
      "Current test: 367/2048\n",
      "Current test: 368/2048\n",
      "Current test: 369/2048\n",
      "Current test: 370/2048\n",
      "Current test: 371/2048\n",
      "Current test: 372/2048\n",
      "Current test: 373/2048\n",
      "Current test: 374/2048\n",
      "Current test: 375/2048\n",
      "Current test: 376/2048\n",
      "Current test: 377/2048\n",
      "Current test: 378/2048\n",
      "Current test: 379/2048\n",
      "Current test: 380/2048\n",
      "Current test: 381/2048\n",
      "Current test: 382/2048\n",
      "Current test: 383/2048\n",
      "Current test: 384/2048\n",
      "Current test: 385/2048\n",
      "Current test: 386/2048\n",
      "Current test: 387/2048\n",
      "Current test: 388/2048\n",
      "Current test: 389/2048\n",
      "Current test: 390/2048\n",
      "Current test: 391/2048\n",
      "Current test: 392/2048\n",
      "Current test: 393/2048\n",
      "Current test: 394/2048\n",
      "Current test: 395/2048\n",
      "Current test: 396/2048\n",
      "Current test: 397/2048\n",
      "Current test: 398/2048\n",
      "Current test: 399/2048\n",
      "Current test: 400/2048\n",
      "Current test: 401/2048\n",
      "Current test: 402/2048\n",
      "Current test: 403/2048\n",
      "Current test: 404/2048\n",
      "Current test: 405/2048\n",
      "Current test: 406/2048\n",
      "Current test: 407/2048\n",
      "Current test: 408/2048\n",
      "Current test: 409/2048\n",
      "Current test: 410/2048\n",
      "Current test: 411/2048\n",
      "Current test: 412/2048\n",
      "Current test: 413/2048\n",
      "Current test: 414/2048\n",
      "Current test: 415/2048\n",
      "Current test: 416/2048\n",
      "Current test: 417/2048\n",
      "Current test: 418/2048\n",
      "Current test: 419/2048\n",
      "Current test: 420/2048\n",
      "Current test: 421/2048\n",
      "Current test: 422/2048\n",
      "Current test: 423/2048\n",
      "Current test: 424/2048\n",
      "Current test: 425/2048\n",
      "Current test: 426/2048\n",
      "Current test: 427/2048\n",
      "Current test: 428/2048\n",
      "Current test: 429/2048\n",
      "Current test: 430/2048\n",
      "Current test: 431/2048\n",
      "Current test: 432/2048\n",
      "Current test: 433/2048\n",
      "Current test: 434/2048\n",
      "Current test: 435/2048\n",
      "Current test: 436/2048\n",
      "Current test: 437/2048\n",
      "Current test: 438/2048\n",
      "Current test: 439/2048\n",
      "Current test: 440/2048\n",
      "Current test: 441/2048\n",
      "Current test: 442/2048\n",
      "Current test: 443/2048\n",
      "Current test: 444/2048\n",
      "Current test: 445/2048\n",
      "Current test: 446/2048\n",
      "Current test: 447/2048\n",
      "Current test: 448/2048\n",
      "Current test: 449/2048\n",
      "Current test: 450/2048\n",
      "Current test: 451/2048\n",
      "Current test: 452/2048\n",
      "Current test: 453/2048\n",
      "Current test: 454/2048\n",
      "Current test: 455/2048\n",
      "Current test: 456/2048\n",
      "Current test: 457/2048\n",
      "Current test: 458/2048\n",
      "Current test: 459/2048\n",
      "Current test: 460/2048\n",
      "Current test: 461/2048\n",
      "Current test: 462/2048\n",
      "Current test: 463/2048\n",
      "Current test: 464/2048\n",
      "Current test: 465/2048\n",
      "Current test: 466/2048\n",
      "Current test: 467/2048\n",
      "Current test: 468/2048\n",
      "Current test: 469/2048\n",
      "Current test: 470/2048\n",
      "Current test: 471/2048\n",
      "Current test: 472/2048\n",
      "Current test: 473/2048\n",
      "Current test: 474/2048\n",
      "Current test: 475/2048\n",
      "Current test: 476/2048\n",
      "Current test: 477/2048\n",
      "Current test: 478/2048\n",
      "Current test: 479/2048\n",
      "Current test: 480/2048\n",
      "Current test: 481/2048\n",
      "Current test: 482/2048\n",
      "Current test: 483/2048\n",
      "Current test: 484/2048\n",
      "Current test: 485/2048\n",
      "Current test: 486/2048\n",
      "Current test: 487/2048\n",
      "Current test: 488/2048\n",
      "Current test: 489/2048\n",
      "Current test: 490/2048\n",
      "Current test: 491/2048\n",
      "Current test: 492/2048\n",
      "Current test: 493/2048\n",
      "Current test: 494/2048\n",
      "Current test: 495/2048\n",
      "Current test: 496/2048\n",
      "Current test: 497/2048\n",
      "Current test: 498/2048\n",
      "Current test: 499/2048\n",
      "Current test: 500/2048\n",
      "Current test: 501/2048\n",
      "Current test: 502/2048\n",
      "Current test: 503/2048\n",
      "Current test: 504/2048\n",
      "Current test: 505/2048\n",
      "Current test: 506/2048\n",
      "Current test: 507/2048\n",
      "Current test: 508/2048\n",
      "Current test: 509/2048\n",
      "Current test: 510/2048\n",
      "Current test: 511/2048\n",
      "Current test: 512/2048\n",
      "Current test: 513/2048\n",
      "Current test: 514/2048\n",
      "Current test: 515/2048\n",
      "Current test: 516/2048\n",
      "Current test: 517/2048\n",
      "Current test: 518/2048\n",
      "Current test: 519/2048\n",
      "Current test: 520/2048\n",
      "Current test: 521/2048\n",
      "Current test: 522/2048\n",
      "Current test: 523/2048\n",
      "Current test: 524/2048\n",
      "Current test: 525/2048\n",
      "Current test: 526/2048\n",
      "Current test: 527/2048\n",
      "Current test: 528/2048\n",
      "Current test: 529/2048\n",
      "Current test: 530/2048\n",
      "Current test: 531/2048\n",
      "Current test: 532/2048\n",
      "Current test: 533/2048\n",
      "Current test: 534/2048\n",
      "Current test: 535/2048\n",
      "Current test: 536/2048\n",
      "Current test: 537/2048\n",
      "Current test: 538/2048\n",
      "Current test: 539/2048\n",
      "Current test: 540/2048\n",
      "Current test: 541/2048\n",
      "Current test: 542/2048\n",
      "Current test: 543/2048\n",
      "Current test: 544/2048\n",
      "Current test: 545/2048\n",
      "Current test: 546/2048\n",
      "Current test: 547/2048\n",
      "Current test: 548/2048\n",
      "Current test: 549/2048\n",
      "Current test: 550/2048\n",
      "Current test: 551/2048\n",
      "Current test: 552/2048\n",
      "Current test: 553/2048\n",
      "Current test: 554/2048\n",
      "Current test: 555/2048\n",
      "Current test: 556/2048\n",
      "Current test: 557/2048\n",
      "Current test: 558/2048\n",
      "Current test: 559/2048\n",
      "Current test: 560/2048\n",
      "Current test: 561/2048\n",
      "Current test: 562/2048\n",
      "Current test: 563/2048\n",
      "Current test: 564/2048\n",
      "Current test: 565/2048\n",
      "Current test: 566/2048\n",
      "Current test: 567/2048\n",
      "Current test: 568/2048\n",
      "Current test: 569/2048\n",
      "Current test: 570/2048\n",
      "Current test: 571/2048\n",
      "Current test: 572/2048\n",
      "Current test: 573/2048\n",
      "Current test: 574/2048\n",
      "Current test: 575/2048\n",
      "Current test: 576/2048\n",
      "Current test: 577/2048\n",
      "Current test: 578/2048\n",
      "Current test: 579/2048\n",
      "Current test: 580/2048\n",
      "Current test: 581/2048\n",
      "Current test: 582/2048\n",
      "Current test: 583/2048\n",
      "Current test: 584/2048\n",
      "Current test: 585/2048\n",
      "Current test: 586/2048\n",
      "Current test: 587/2048\n",
      "Current test: 588/2048\n",
      "Current test: 589/2048\n",
      "Current test: 590/2048\n",
      "Current test: 591/2048\n",
      "Current test: 592/2048\n",
      "Current test: 593/2048\n",
      "Current test: 594/2048\n",
      "Current test: 595/2048\n",
      "Current test: 596/2048\n",
      "Current test: 597/2048\n",
      "Current test: 598/2048\n",
      "Current test: 599/2048\n",
      "Current test: 600/2048\n",
      "Current test: 601/2048\n",
      "Current test: 602/2048\n",
      "Current test: 603/2048\n",
      "Current test: 604/2048\n",
      "Current test: 605/2048\n",
      "Current test: 606/2048\n",
      "Current test: 607/2048\n",
      "Current test: 608/2048\n",
      "Current test: 609/2048\n",
      "Current test: 610/2048\n",
      "Current test: 611/2048\n",
      "Current test: 612/2048\n",
      "Current test: 613/2048\n",
      "Current test: 614/2048\n",
      "Current test: 615/2048\n",
      "Current test: 616/2048\n",
      "Current test: 617/2048\n",
      "Current test: 618/2048\n",
      "Current test: 619/2048\n",
      "Current test: 620/2048\n",
      "Current test: 621/2048\n",
      "Current test: 622/2048\n",
      "Current test: 623/2048\n",
      "Current test: 624/2048\n",
      "Current test: 625/2048\n",
      "Current test: 626/2048\n",
      "Current test: 627/2048\n",
      "Current test: 628/2048\n",
      "Current test: 629/2048\n",
      "Current test: 630/2048\n",
      "Current test: 631/2048\n",
      "Current test: 632/2048\n",
      "Current test: 633/2048\n",
      "Current test: 634/2048\n",
      "Current test: 635/2048\n",
      "Current test: 636/2048\n",
      "Current test: 637/2048\n",
      "Current test: 638/2048\n",
      "Current test: 639/2048\n",
      "Current test: 640/2048\n",
      "Current test: 641/2048\n",
      "Current test: 642/2048\n",
      "Current test: 643/2048\n",
      "Current test: 644/2048\n",
      "Current test: 645/2048\n",
      "Current test: 646/2048\n",
      "Current test: 647/2048\n",
      "Current test: 648/2048\n",
      "Current test: 649/2048\n",
      "Current test: 650/2048\n",
      "Current test: 651/2048\n",
      "Current test: 652/2048\n",
      "Current test: 653/2048\n",
      "Current test: 654/2048\n",
      "Current test: 655/2048\n",
      "Current test: 656/2048\n",
      "Current test: 657/2048\n",
      "Current test: 658/2048\n",
      "Current test: 659/2048\n",
      "Current test: 660/2048\n",
      "Current test: 661/2048\n",
      "Current test: 662/2048\n",
      "Current test: 663/2048\n",
      "Current test: 664/2048\n",
      "Current test: 665/2048\n",
      "Current test: 666/2048\n",
      "Current test: 667/2048\n",
      "Current test: 668/2048\n",
      "Current test: 669/2048\n",
      "Current test: 670/2048\n",
      "Current test: 671/2048\n",
      "Current test: 672/2048\n",
      "Current test: 673/2048\n",
      "Current test: 674/2048\n",
      "Current test: 675/2048\n",
      "Current test: 676/2048\n",
      "Current test: 677/2048\n",
      "Current test: 678/2048\n",
      "Current test: 679/2048\n",
      "Current test: 680/2048\n",
      "Current test: 681/2048\n",
      "Current test: 682/2048\n",
      "Current test: 683/2048\n",
      "Current test: 684/2048\n",
      "Current test: 685/2048\n",
      "Current test: 686/2048\n",
      "Current test: 687/2048\n",
      "Current test: 688/2048\n",
      "Current test: 689/2048\n",
      "Current test: 690/2048\n",
      "Current test: 691/2048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-d1df13ed57bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vector_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vector_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresult_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-227-243ba625c271>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(x_train, score_train, x_test, y_gold)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mevaluate_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/v2john/.env/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1028\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/v2john/.env/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1082\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/v2john/.env/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 787\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/v2john/.env/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/v2john/.env/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for emotion in ['sadness', 'joy', 'anger', 'fear']:\n",
    "\n",
    "    print(\"Working on: \" + emotion)\n",
    "    tweet_train, tweet_test, score_train, y_gold = load_all_data(emotion)\n",
    "    \n",
    "    result_file_path = \"/home/v2john/\" + emotion + \"_tests.tsv\"\n",
    "    with open(result_file_path, 'a+') as result_file:\n",
    "        result_file.write(\n",
    "            \"Feature Selection String\\t\" +\n",
    "            \"Num. Features\\t\" + \n",
    "            \"Training Pearson Co-efficient\\t\" +\n",
    "            \"Training Spearman Co-efficient\\t\" + \n",
    "            \"Training Pearson Co-efficient (0.5-1)\\t\" + \n",
    "            \"Training Spearman Co-efficient (0.5-1)\\t\" +\n",
    "            \"Test Pearson Co-efficient\\t\" + \n",
    "            \"Test Spearman Co-efficient\\t\" + \n",
    "            \"Test Pearson Co-efficient (0.5-1)\\t\" +\n",
    "            \"Test Spearman Co-efficient (0.5-1)\\n\"\n",
    "        )\n",
    "\n",
    "    train_vector_dict = dict()\n",
    "    test_vector_dict = dict()\n",
    "    \n",
    "    for i in range(1, num_test + 1):\n",
    "        print(\"Current test: \" + str(i) + \"/\" + str(num_test))\n",
    "        bin_string = '{0:011b}'.format(i)\n",
    "\n",
    "        x_train = vectorize_tweets(tweet_train, bin_string, train_vector_dict)\n",
    "        x_test = vectorize_tweets(tweet_test, bin_string, test_vector_dict)\n",
    "        train_scores, test_scores = run_test(x_train, score_train, x_test, y_gold)\n",
    "        \n",
    "        with open(result_file_path, 'a+') as result_file:\n",
    "            result_file.write(\n",
    "                \"~\" + bin_string + \"\\t\" + \n",
    "                str(len(x_train[0])) + \"\\t\" +\n",
    "                str(train_scores[0]) + \"\\t\" + \n",
    "                str(train_scores[1]) + \"\\t\" + \n",
    "                str(train_scores[2]) + \"\\t\" + \n",
    "                str(train_scores[3]) + \"\\t\" +\n",
    "                str(test_scores[0]) + \"\\t\" + \n",
    "                str(test_scores[1]) + \"\\t\" + \n",
    "                str(test_scores[2]) + \"\\t\" + \n",
    "                str(test_scores[3]) + \"\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Implementation in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "_, dim_size = (np.array(x_train).shape)\n",
    "print(dim_size)\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, activation='relu', input_dim=dim_size))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(33, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=1000, batch_size=5, verbose=0)\n",
    "estimator.fit(x_train, score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = estimator.predict(x_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(predictions_file_path, 'w') as predictions_file:\n",
    "    for i in range(len(y_test)):\n",
    "        predictions_file.write(\n",
    "            str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "            test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
