{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import scipy.stats\n",
    "\n",
    "def evaluate(pred,gold):\n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "        \n",
    "        for line in gold_lines:\n",
    "            line = line.decode()\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:   \n",
    "                data_dic[int(parts[0])]=[float(line.split('\\t')[3])]\n",
    "            else:\n",
    "                raise ValueError('Format problem.')\n",
    "        \n",
    "        \n",
    "        for line in pred_lines:\n",
    "            line = line.decode()\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:  \n",
    "                if int(parts[0]) in data_dic:\n",
    "                    try:\n",
    "                        data_dic[int(parts[0])].append(float(line.split('\\t')[3]))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[int(parts[0])].append(0.5)\n",
    "                else:\n",
    "                    raise ValueError('Invalid tweet id.')\n",
    "            else:\n",
    "                raise ValueError('Format problem.')\n",
    "            \n",
    "            \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "         \n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                if(data_dic[id][0]>=0.5):\n",
    "                    gold_scores_range_05_1.append(data_dic[id][0])\n",
    "                    pred_scores_range_05_1.append(data_dic[id][1])\n",
    "            else:\n",
    "                raise ValueError('Repeated id in test data.')\n",
    "                \n",
    "      \n",
    "        # return zero correlation if predictions are constant\n",
    "        if numpy.std(pred_scores)==0 or numpy.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                    \n",
    "        spear_corr=scipy.stats.spearmanr(pred_scores,gold_scores)[0]   \n",
    "\n",
    "\n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                                    \n",
    "        spear_corr_range_05_1=scipy.stats.spearmanr(pred_scores_range_05_1,gold_scores_range_05_1)[0]           \n",
    "        \n",
    "      \n",
    "        return (pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1)\n",
    "    else:\n",
    "        raise ValueError('Predictions and gold data have different number of lines.')\n",
    "        \n",
    "def evaluate_lists(pred, gold):\n",
    "    if len(pred) == len(gold):\n",
    "        gold_scores=gold\n",
    "        pred_scores=pred         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "         \n",
    "            \n",
    "        for i in range(len(gold_scores)):\n",
    "            if(gold_scores[i]>=0.5):\n",
    "                gold_scores_range_05_1.append(gold_scores[i])\n",
    "                pred_scores_range_05_1.append(pred_scores[i])\n",
    "                \n",
    "        # return zero correlation if predictions are constant\n",
    "        if numpy.std(pred_scores)==0 or numpy.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                    \n",
    "        spear_corr=scipy.stats.spearmanr(pred_scores,gold_scores)[0]   \n",
    "\n",
    "\n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                                    \n",
    "        spear_corr_range_05_1=scipy.stats.spearmanr(pred_scores_range_05_1,gold_scores_range_05_1)[0]           \n",
    "        \n",
    "      \n",
    "        return np.array([pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1])\n",
    "    else:\n",
    "        raise ValueError('Predictions and gold data have different number of lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector_path = \"/home/v2john/\"\n",
    "wassa_home = \"/home/v2john/WASSA-Task/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google news pretrained vectors\n",
    "wv_model_path = word_vector_path + \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter pretrained vectors\n",
    "wv_model_path_1 = word_vector_path + \"word2vec_twitter_model.bin\"\n",
    "wv_model_1 = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path_1, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = np.array(embedding)\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "wv_model_path_2 = word_vector_path + \"glove.twitter.27B.200d.txt\"\n",
    "wv_model_2 = loadGloveModel(wv_model_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dimensions = len(wv_model['word'])\n",
    "w2v_dimensions_1 = len(wv_model_1['word'])\n",
    "w2v_dimensions_2 = len(wv_model_2['word'])\n",
    "print(w2v_dimensions, w2v_dimensions_1, w2v_dimensions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "# bi_tokens = list(bigrams(word_tokenize(\"This is a sample sentence!!!\")))\n",
    "# for bi_token in bi_tokens:\n",
    "#     print(\" \".join(bi_token))\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "        [word for word in string.split()\n",
    "         if word not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synsetlist = list(swn.senti_synsets('super'))\n",
    "# print(synsetlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_str(string):  \n",
    "    string = html.unescape(string)\n",
    "    string = string.replace(\"\\\\n\", \" \")\n",
    "    string = string.replace(\"_NEG\", \"\")\n",
    "    string = string.replace(\"_NEGFIRST\", \"\")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string) # removing any twitter handle mentions\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\*\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" ,\", string)\n",
    "    string = re.sub(r\"!\", \" !\", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return remove_stopwords(string.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata and Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, intensity):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "            \"id: \" + self.id + \\\n",
    "            \", text: \" + self.text + \\\n",
    "            \", emotion: \" + self.emotion + \\\n",
    "            \", intensity: \" + self.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_training_data(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], clean_str(array[1]), array[2], float(array[3])))\n",
    "    return train_list\n",
    "            \n",
    "def read_test_data(training_data_file_path):\n",
    "\n",
    "    test_list = list()\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            test_list.append(Tweet(array[0], clean_str(array[1]), array[2], None))\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion = \"sadness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_file_path = \\\n",
    "    wassa_home + \"dataset/\" + \\\n",
    "    emotion + \"-ratings-0to1.train.txt\"\n",
    "test_data_file_path = \\\n",
    "    wassa_home + \"dataset/\" + \\\n",
    "    emotion + \"-ratings-0to1.dev.target.txt\"\n",
    "predictions_file_path = \\\n",
    "    wassa_home + \"predictions/\" + \\\n",
    "    emotion + \"-pred.txt\"\n",
    "gold_set_path = \\\n",
    "    wassa_home + \"dataset/gold-set/\" + \\\n",
    "    emotion + \"-ratings-0to1.dev.gold.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Intensity Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affect_intensity_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-AffectIntensity-Lexicon.txt\"\n",
    "\n",
    "def get_word_affect_intensity_dict(emotion):\n",
    "    word_intensities = dict()\n",
    "\n",
    "    with open(affect_intensity_file_path) as affect_intensity_file:\n",
    "        for line in affect_intensity_file:\n",
    "            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_int_array[2] == emotion):\n",
    "                word_intensities[word_int_array[0]] = float(word_int_array[1])\n",
    "\n",
    "    return word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_intensities = get_word_affect_intensity_dict(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "\n",
    "def get_emo_int_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in word_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(word_intensities[word])\n",
    "    \n",
    "    return poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(tweet, model, dimensions):\n",
    "    vector_list = list()\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vector_list.append(model[word])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if len(vector_list) == 0:\n",
    "        vec_rep = np.zeros(dimensions).tolist()\n",
    "    else:\n",
    "        try:\n",
    "            vec_rep = sum(vector_list) / float(len(vector_list))\n",
    "        except Exception as e:\n",
    "            print(vector_list)\n",
    "            print(e)\n",
    "            raise Exception\n",
    "\n",
    "    return vec_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_sentiwordnet = PolynomialFeatures(5)\n",
    "\n",
    "def get_sentiwordnetscore(tweet):\n",
    "    \n",
    "    tweet_score = np.zeros(2)\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        synsetlist = list(swn.senti_synsets(word))\n",
    "        \n",
    "        if synsetlist:\n",
    "            tweet_score[0] += synsetlist[0].pos_score()\n",
    "            tweet_score[1] += synsetlist[0].neg_score()\n",
    "            \n",
    "    sentiwordnetscore_list = poly_sentiwordnet.fit_transform(tweet_score.reshape(1, -1))[0].tolist()\n",
    "    \n",
    "    return sentiwordnetscore_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Emotion Presence Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "def get_affect_presence_list(emotion):\n",
    "    word_list = list()\n",
    "    \n",
    "    with open(sentiment_emotion_lex_file_path) as sentiment_emotion_lex_file:\n",
    "        for line in sentiment_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[1] == emotion and word_array[2] == '1'):\n",
    "                word_list.append(word_array[0])\n",
    "                \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = get_affect_presence_list(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_emotion_feature(tweet):\n",
    "    for word in word_list:\n",
    "        if word in tweet.split():\n",
    "            return [1.0]\n",
    "    \n",
    "    return [0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_emotion_lex_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2/\" + \\\n",
    "    \"NRC-Hashtag-Emotion-Lexicon-v0.2.txt\"\n",
    "    \n",
    "def get_hashtag_emotion_intensity(emotion):\n",
    "    hastag_intensities = dict()\n",
    "    \n",
    "    with open(hashtag_emotion_lex_file_path) as hashtag_emotion_lex_file:\n",
    "        for line in hashtag_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[0] == emotion):\n",
    "                hastag_intensities[clean_str(word_array[1])] = float(word_array[2])\n",
    "                \n",
    "    return hastag_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_emotion_intensities = get_hashtag_emotion_intensity(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "\n",
    "def get_hashtag_emotion_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in hashtag_emotion_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(hashtag_emotion_intensities[word])\n",
    "    \n",
    "    return poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticon Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-unigrams.txt\"\n",
    "emoticon_lexicon_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-bigrams.txt\"\n",
    "emoticon_lexicon_pairs_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-pairs.txt\"\n",
    "pair_split_string = \"---\"\n",
    "    \n",
    "emoticon_lexicon_unigrams = dict()\n",
    "emoticon_lexicon_bigrams = dict()\n",
    "emoticon_lexicon_pairs = dict()\n",
    "\n",
    "def get_emoticon_lexicon_unigram_dict():\n",
    "    with open(emoticon_lexicon_unigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_unigrams\n",
    "\n",
    "def get_emoticon_lexicon_bigram_dict():\n",
    "    with open(emoticon_lexicon_bigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_bigrams\n",
    "\n",
    "def get_emoticon_lexicon_pairs_dict():\n",
    "    with open(emoticon_lexicon_pairs_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in emoticon_lexicon_pairs.keys():\n",
    "                    token_1_dict = emoticon_lexicon_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                emoticon_lexicon_pairs[token_1] = token_1_dict\n",
    "    \n",
    "    return emoticon_lexicon_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigram_dict = get_emoticon_lexicon_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_lexicon_bigram_dict = get_emoticon_lexicon_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_pairs_dict = get_emoticon_lexicon_pairs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(emoticon_lexicon_pairs_dict))\n",
    "# random_key = list(emoticon_lexicon_pairs_dict.keys())[0]\n",
    "# print(random_key)\n",
    "# print(emoticon_lexicon_pairs_dict[random_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emoticon_lexicon = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in emoticon_lexicon_unigram_dict.keys():\n",
    "            vector_list += emoticon_lexicon_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_bigram_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in emoticon_lexicon_bigram_dict.keys():\n",
    "            vector_list += emoticon_lexicon_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_pair_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        word_1 = clean_str(tokens[i])\n",
    "        if word_1 in emoticon_lexicon_pairs_dict.keys():\n",
    "            token_1_dict = emoticon_lexicon_pairs_dict[word_1]\n",
    "            for j in range(i, len(tokens)):\n",
    "                word_2 = clean_str(tokens[j])\n",
    "                if word_2 in token_1_dict.keys():\n",
    "                    vector_list += token_1_dict[word_2]\n",
    "                    counter += 1\n",
    "                    \n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_emoticon_lexicon_vector(tweet):\n",
    "    final_list = list()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_emoticon_lexicon_vector(tokens))\n",
    "    \n",
    "    # Adding bigram features\n",
    "    final_list.extend(get_bigram_sentiment_emoticon_lexicon_vector(tokens))\n",
    "    \n",
    "    # Adding pair features\n",
    "    final_list.extend(get_pair_sentiment_emoticon_lexicon_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticon Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_afflex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-unigrams.txt\"\n",
    "emoticon_afflex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emoticon-Lexicon-v1.0/Emoticon-bigrams.txt\"\n",
    "    \n",
    "emoticon_afflex_unigrams = dict()\n",
    "emoticon_afflex_bigrams = dict()\n",
    "\n",
    "def get_emoticon_afflex_unigram_dict():\n",
    "    with open(emoticon_afflex_unigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_unigrams\n",
    "\n",
    "def get_emoticon_afflex_bigram_dict():\n",
    "    with open(emoticon_afflex_bigrams_file_path) as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_afflex_unigram_dict = get_emoticon_afflex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_afflex_bigram_dict = get_emoticon_afflex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emoticon_lexicon = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_emoticon_afflex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in emoticon_afflex_unigram_dict.keys():\n",
    "            vector_list += emoticon_afflex_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "\n",
    "def get_bigram_sentiment_emoticon_afflex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in emoticon_afflex_bigram_dict.keys():\n",
    "            vector_list += emoticon_afflex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_emoticon_afflex_vector(tweet):\n",
    "    final_list = list()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_emoticon_afflex_vector(tokens))\n",
    "    \n",
    "    # Adding bigram featunigram_list =ures\n",
    "    final_list.extend(get_bigram_sentiment_emoticon_afflex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-AffLexNegLex-v1.0/\" + \\\n",
    "    \"HS-AFFLEX-NEGLEX-unigrams.txt\"\n",
    "hashtag_affneglex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-AffLexNegLex-v1.0/\" + \\\n",
    "    \"HS-AFFLEX-NEGLEX-bigrams.txt\"\n",
    "    \n",
    "hashtag_affneglex_unigrams = dict()\n",
    "hashtag_affneglex_bigrams = dict()\n",
    "\n",
    "def get_hashtag_affneglex_unigram_dict():\n",
    "    with open(hashtag_affneglex_unigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_unigrams[clean_str(word_array[0])] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hashtag_affneglex_unigrams\n",
    "\n",
    "def get_hashtag_affneglex_bigram_dict():\n",
    "    with open(hashtag_affneglex_bigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_bigrams[clean_str(word_array[0])] = np.array([float(val) for val in word_array[1:]])\n",
    "\n",
    "    return hashtag_affneglex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigram_dict = get_hashtag_affneglex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_affneglex_bigram_dict = get_hashtag_affneglex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_hashtag_sent_affneglex = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_hashtag_affneglex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in hashtag_affneglex_unigram_dict.keys():\n",
    "            vector_list += hashtag_affneglex_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_hashtag_sent_affneglex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_bigram_sentiment_hashtag_affneglex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in hashtag_affneglex_bigram_dict.keys():\n",
    "            vector_list += hashtag_affneglex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_hashtag_sent_affneglex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_hashtag_affneglex_vector(tweet):\n",
    "    final_list = list()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_hashtag_affneglex_vector(tokens))\n",
    "    # Adding bigram features\n",
    "    final_list.extend(get_bigram_sentiment_hashtag_affneglex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_unigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-Lexicon-v1.0/HS-unigrams.txt\"\n",
    "hash_sent_lex_bigrams_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-Lexicon-v1.0/HS-bigrams.txt\"\n",
    "hash_sent_lex_pairs_file_path = \\\n",
    "    wassa_home + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Sentiment-Lexicon-v1.0/HS-pairs.txt\"\n",
    "pair_split_string = \"---\"\n",
    "\n",
    "hash_sent_lex_unigrams = dict()\n",
    "hash_sent_lex_bigrams = dict()\n",
    "hash_sent_lex_pairs = dict()\n",
    "\n",
    "def get_hash_sent_lex_unigram_dict():\n",
    "    with open(hash_sent_lex_unigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_unigrams[word_array[0]] = np.arr\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in hash_sent_lex_pairs.keys():\n",
    "                    token_1_dict = hash_sent_lex_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                hash_sent_lex_pairs[token_1] = token_1_dictay([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_unigrams\n",
    "\n",
    "def get_hash_sent_lex_bigram_dict():\n",
    "    with open(hash_sent_lex_bigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_bigrams\n",
    "\n",
    "def get_hash_sent_lex_pairs_dict():\n",
    "    with open(hash_sent_lex_pairs_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in hash_sent_lex_pairs.keys():\n",
    "                    token_1_dict = hash_sent_lex_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                hash_sent_lex_pairs[token_1] = token_1_dict\n",
    "    \n",
    "    return hash_sent_lex_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_unigram_dict = get_hash_sent_lex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_bigram_dict = get_hash_sent_lex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_sent_lex_pairs_dict = get_hash_sent_lex_pairs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_hash_sent_lex = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_hash_sent_lex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in hash_sent_lex_unigram_dict.keys():\n",
    "            vector_list += hash_sent_lex_unigram_dict[word]\n",
    "            counter += 1\n",
    "\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    \n",
    "    return poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "    \n",
    "def get_bigram_sentiment_hash_sent_lex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in hash_sent_lex_bigram_dict.keys():\n",
    "            vector_list += hash_sent_lex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    \n",
    "    return poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_pair_sentiment_hash_sent_lex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        word_1 = clean_str(tokens[i])\n",
    "        if word_1 in hash_sent_lex_pairs_dict.keys():\n",
    "            token_1_dict = hash_sent_lex_pairs_dict[word_1]\n",
    "            for j in range(i, len(tokens)):\n",
    "                word_2 = clean_str(tokens[j])\n",
    "                if word_2 in token_1_dict.keys():\n",
    "                    vector_list += token_1_dict[word_2]\n",
    "                    counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    return poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].tolist()\n",
    "    \n",
    "def get_sentiment_hash_sent_lex_vector(tweet):\n",
    "    final_list = list()\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list.extend(get_unigram_sentiment_hash_sent_lex_vector(tokens))\n",
    "    # Adding bigram features\n",
    "#     final_list.extend(get_bigram_sentiment_hash_sent_lex_vector(tokens))\n",
    "    # Adding pair features\n",
    "#     final_list.extend(get_pair_sentiment_hash_sent_lex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n"
     ]
    }
   ],
   "source": [
    "training_tweets = read_training_data(training_data_file_path)\n",
    "\n",
    "with open(training_data_file_path + \".cleaned\", 'w') as cleaned_input_file:\n",
    "    for tweet in training_tweets:\n",
    "        cleaned_input_file.write(tweet.id + \"\\t\" + tweet.text + \"\\n\")\n",
    "\n",
    "score_train = list()\n",
    "tweet_train = list()\n",
    "for tweet in training_tweets:\n",
    "    tweet_train.append(tweet.text)\n",
    "    score_train.append(float(tweet.intensity))\n",
    "print(len(score_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_tweets(tweet_list):\n",
    "    vectors = list()\n",
    "\n",
    "    for i in range(len(tweet_list)):\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"Vectorizing tweet \" + str(i))\n",
    "        \n",
    "        x_vector = list()\n",
    "#         x_vector.extend(get_word2vec_embedding(tweet_list[i], wv_model, w2v_dimensions))\n",
    "#         x_vector.extend(get_word2vec_embedding(tweet_list[i], wv_model_1, w2v_dimensions_1))\n",
    "#         x_vector.extend(get_word2vec_embedding(tweet_list[i], wv_model_2, w2v_dimensions_2))\n",
    "#         x_vector.extend(get_emo_int_vector(tweet_list[i]))\n",
    "#         x_vector.extend(get_sentiwordnetscore(tweet_list[i]))\n",
    "#         x_vector.extend(get_sentiment_emotion_feature(tweet_list[i]))\n",
    "#         x_vector.extend(get_hashtag_emotion_vector(tweet_list[i]))\n",
    "        x_vector.extend(get_sentiment_emoticon_lexicon_vector(tweet_list[i]))\n",
    "#         x_vector.extend(get_sentiment_emoticon_afflex_vector(tweet_list[i]))\n",
    "#         x_vector.extend(get_sentiment_hashtag_affneglex_vector(tweet_list[i]))\n",
    "#         x_vector.extend(get_sentiment_hash_sent_lex_vector(tweet_list[i]))\n",
    "        vectors.append(x_vector)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing tweet 0\n",
      "Vectorizing tweet 100\n",
      "Vectorizing tweet 200\n",
      "Vectorizing tweet 300\n",
      "Vectorizing tweet 400\n",
      "Vectorizing tweet 500\n",
      "Vectorizing tweet 600\n",
      "Vectorizing tweet 700\n",
      "786\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorize_tweets(tweet_train)\n",
    "print(len(x_train))\n",
    "dimension = len(x_train[0])\n",
    "print(dimension)\n",
    "\n",
    "# with open(\"/tmp/dump.txt\", 'w') as dump_file:\n",
    "#     for i in range(len(x_train)):\n",
    "#         if dimension != len(x_train[i]):\n",
    "#             print(len(x_train[i]), i)\n",
    "#         dump_file.write(str(x_train[i]))\n",
    "#         dump_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, -1.1012499999999998, 44579.75, 31772.75, 1.2127515624999996, -49093.44968749999, -34989.74093749999, 1987354110.0625, 1416421251.8125, 1009507642.5625, -1.3355426582031245, 54064.16146835936, 38532.45220742186, -2188573713.7063274, -1559833903.5585153, -1111720291.3719528, 88595749388058.73, 63143705300488.3, 45003598328525.61, 32074833950227.67, 1.4707663523461907, -59538.15781703074, -42433.862993423325, 2410166802.2190933, 1717767086.293815, 1224281970.873363, -97566069013599.66, -69537005462162.72, -49560212659288.82, -35322410887688.21, 3.9495763587823114e+18, 2.8149305963694433e+18, 2.0062491625860895e+18, 1.429888078792662e+18, 1.0191056803920963e+18, -1.6196814455212423, 65566.3962960051, 46730.29162150743, -2654196190.943776, -1891691003.7810636, -1348240520.424291, 107444633501226.62, 76577627265206.7, 54578184191041.805, 38898804990066.65, -4.349470965109019e+18, -3.0999423192518487e+18, -2.2093818902979305e+18, -1.574664246770419e+18, -1.1222901305317956e+18, 1.7607112668042576e+23, 1.2548890225350068e+23, 8.943808610579723e+22, 6.374405308055718e+22, 4.543147645545955e+22, 3.2379790006677976e+22, 1.0, -1.2366666666666666, 42.333333333333336, 331.0, 1.5293444444444442, -52.352222222222224, -409.33666666666664, 1792.1111111111113, 14012.333333333334, 109561.0, -1.8912892962962957, 64.74224814814814, 506.21301111111103, -2216.2440740740744, -17328.585555555557, -135490.43666666665, 75866.03703703705, 593188.7777777779, 4638082.333333334, 36264691.0, 2.3388944297530854, -80.06458020987652, -626.0167570740739, 2740.755171604938, 21429.68413703703, 167556.50667777774, -93820.99913580249, -733576.7885185187, -5735761.81888889, -44847334.53666666, 3211662.234567902, 25111658.259259265, 196345485.44444448, 1535205252.3333335, 12003612721.0, -2.892432778127982, 99.01319752621396, 774.1740562482713, -3389.400562218106, -26501.37604946913, -207211.54659151845, 116025.30226460904, 907189.9618012344, 7093225.449359258, 55461203.710344434, -3971755.6300823055, -31054750.713950623, -242813916.99962968, -1898537162.0522225, -14844467731.636665, 135960367.9300412, 1063060199.6419755, 8311958883.814816, 64990355682.11112, 508152938522.3334, 3973195810651.0, 1.0, -4.999, 0.6666666666666666, 6.833333333333333, 24.990000999999996, -3.3326666666666664, -34.15983333333333, 0.4444444444444444, 4.555555555555555, 46.69444444444444, -124.92501499899997, 16.66000066666666, 170.7650068333333, -2.2217777777777776, -22.77322222222222, -233.42552777777775, 0.2962962962962963, 3.0370370370370368, 31.129629629629626, 319.0787037037037, 624.5001499800009, -83.28334333266665, -853.6542691598331, 11.106667111111108, 113.84333788888885, 1166.8942133611108, -1.481185185185185, -15.182148148148146, -155.6170185185185, -1595.0744398148145, 0.19753086419753085, 2.024691358024691, 20.753086419753085, 212.7191358024691, 2180.3711419753085, -3121.8762497500243, 416.33343332000055, 4267.417691530006, -55.52222888844443, -569.1028461065554, -5833.304172592193, 7.404444740740738, 75.89555859259256, 777.9294755740738, 7973.777124634257, -0.9874567901234566, -10.12143209876543, -103.74467901234566, -1063.382959876543, -10899.675338734565, 0.1316872427983539, 1.3497942386831274, 13.835390946502054, 141.81275720164606, 1453.5807613168722, 14899.202803497941]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing tweet 0\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "test_tweets = read_test_data(test_data_file_path)\n",
    "with open(test_data_file_path + \".cleaned\", 'w') as cleaned_input_file:\n",
    "    for tweet in test_tweets:\n",
    "        cleaned_input_file.write(tweet.id + \"\\t\" + tweet.text + \"\\n\")\n",
    "\n",
    "tweet_test = list()\n",
    "for tweet in test_tweets:\n",
    "    tweet_test.append(tweet.text)\n",
    "\n",
    "x_test = vectorize_tweets(tweet_test)\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### sadness\n",
      "| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\n",
      "| --- | --- | --- | --- |\n",
      "| 0.560408136441 | 0.55353645987 | 0.328826812316 | 0.307461319352 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble, svm, model_selection\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "ml_model = ensemble.GradientBoostingRegressor(max_depth=3, n_estimators=100)\n",
    "# ml_model = ensemble.AdaBoostRegressor()\n",
    "# ml_model = XGBRegressor(max_depth=1, n_estimators=100)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "score_train = np.array(score_train)\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores = np.zeros(4)\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "    y_train, y_test = score_train[train_index], score_train[test_index]\n",
    "    ml_model.fit(X_train, y_train)\n",
    "    y_pred = ml_model.predict(X_test)\n",
    "    scores += evaluate_lists(y_pred, y_test)\n",
    "\n",
    "avg_scores = scores/5\n",
    "print(\"### \" + emotion)\n",
    "print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "print(\"| --- | --- | --- | --- |\")\n",
    "print(\"| \" + str(avg_scores[0]) + \" | \" + str(avg_scores[1]) + \" | \" + \\\n",
    "      str(avg_scores[2]) + \" | \" + str(avg_scores[3]) + \" |\")\n",
    "\n",
    "ml_model.fit(x_train, score_train)\n",
    "\n",
    "y_test = ml_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0513240455264\n"
     ]
    }
   ],
   "source": [
    "y_gold = read_training_data(gold_set_path)\n",
    "\n",
    "data_dict = dict()\n",
    "diff = 0\n",
    "for i in range(len(y_gold)):\n",
    "    if y_gold[i].intensity >= 0.5:\n",
    "        diff += y_gold[i].intensity - y_test[i]\n",
    "#         print([tweet_test[i], str(y_test[i]), str(y_gold[i].intensity)])\n",
    "print(diff/len(y_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(predictions_file_path, 'w') as predictions_file:\n",
    "    for i in range(len(y_test)):\n",
    "        predictions_file.write(\n",
    "            str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "            test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation based on Pearson and Spearman co-efficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### sadness\n",
      "| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\n",
      "| --- | --- | --- | --- |\n",
      "| 0.532916549661 | 0.532911581855 | 0.401649412521 | 0.352193817126 |\n"
     ]
    }
   ],
   "source": [
    "print(\"### \" + emotion)\n",
    "print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "print(\"| --- | --- | --- | --- |\")\n",
    "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1 = \\\n",
    "    evaluate(predictions_file_path, gold_set_path)\n",
    "print(\"| \" + str(pears_corr) + \" | \" + str(spear_corr) + \" | \" + \\\n",
    "      str(pears_corr_range_05_1) + \" | \" + str(spear_corr_range_05_1) + \" |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overall Score Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pears_corr_sum = 0\n",
    "spear_corr_sum = 0\n",
    "pears_corr_range_05_1_sum = 0\n",
    "spear_corr_range_05_1_sum = 0\n",
    "\n",
    "for emotion in ['anger', 'fear', 'sadness', 'joy']:\n",
    "    print(\"\\n### \" + emotion)\n",
    "    predictions_file_path = \\\n",
    "        wassa_home + \"predictions/\" + \\\n",
    "        emotion + \"-pred.txt\"\n",
    "    gold_set_path = \\\n",
    "        wassa_home + \"dataset/gold-set/\" + \\\n",
    "        emotion + \"-ratings-0to1.dev.gold.txt\"\n",
    "    print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "    print(\"| --- | --- | --- | --- |\")\n",
    "    pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1 = \\\n",
    "        evaluate(predictions_file_path, gold_set_path)\n",
    "    print(\"| \" + str(pears_corr) + \" | \" + str(spear_corr) + \" | \" + \\\n",
    "          str(pears_corr_range_05_1) + \" | \" + str(spear_corr_range_05_1) + \" |\")\n",
    "    pears_corr_sum += pears_corr\n",
    "    spear_corr_sum += spear_corr\n",
    "    pears_corr_range_05_1_sum += pears_corr_range_05_1\n",
    "    spear_corr_range_05_1_sum += spear_corr_range_05_1\n",
    "    \n",
    "print(\"\\n### Average Scores\")\n",
    "print(\"| pears-corr | spear-corr | pears-corr-range-05-1 | spear-corr-range-05-1 |\")\n",
    "print(\"| --- | --- | --- | --- |\")\n",
    "print(\"| \" + str(pears_corr_sum/4) + \" | \" + str(spear_corr_sum/4) + \" | \" + \\\n",
    "      str(pears_corr_range_05_1_sum/4) + \" | \" + str(spear_corr_range_05_1_sum/4) + \" |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Implementation in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "_, dim_size = (np.array(x_train).shape)\n",
    "print(dim_size)\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, activation='relu', input_dim=dim_size))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(33, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=1000, batch_size=5, verbose=0)\n",
    "estimator.fit(x_train, score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = estimator.predict(x_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(predictions_file_path, 'w') as predictions_file:\n",
    "    for i in range(len(y_test)):\n",
    "        predictions_file.write(\n",
    "            str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "            test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
