{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_model_path = \"/home/v2john/Documents/GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load affect intensity lexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_intensity_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/\" + \\\n",
    "    \"lexicons/NRC-AffectIntensity-Lexicon.txt\"\n",
    "\n",
    "def get_word_affect_intensity_dict(emotion):\n",
    "    word_intensities = dict()\n",
    "\n",
    "    with open(affect_intensity_file_path) as affect_intensity_file:\n",
    "        for line in affect_intensity_file:\n",
    "            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_int_array[2] == emotion):\n",
    "                word_intensities[word_int_array[0]] = float(word_int_array[1])\n",
    "\n",
    "    return word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/\" + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Emotion-Lexicon-v0.92/\" + \\\n",
    "    \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "def get_affect_presence_list(emotion):\n",
    "    word_list = list()\n",
    "    \n",
    "    with open(sentiment_emotion_lex_file_path) as sentiment_emotion_lex_file:\n",
    "        for line in sentiment_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[1] == emotion and word_array[2] == '1'):\n",
    "                word_list.append(word_array[0])\n",
    "                \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_emotion_lex_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/\" + \\\n",
    "    \"lexicons/NRC-Sentiment-Emotion-Lexicons/Lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2/\" + \\\n",
    "    \"NRC-Hashtag-Emotion-Lexicon-v0.2.txt\"\n",
    "    \n",
    "def get_hashtag_emotion_intensity(emotion):\n",
    "    hastag_intensities = dict()\n",
    "    \n",
    "    with open(hashtag_emotion_lex_file_path) as hashtag_emotion_lex_file:\n",
    "        for line in hashtag_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[0] == emotion):\n",
    "                hastag_intensities[word_array[1].replace(\"#\", \"\")] = float(word_array[2])\n",
    "                \n",
    "    return hastag_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "        [word for word in string.split()\n",
    "         if word not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string) # removing any twitter handle mentions\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return remove_stopwords(string.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "night depression winning depression fml help\n"
     ]
    }
   ],
   "source": [
    "print(clean_str(\"A night where depression is winning... #depression #fml #help\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, intensity):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "            \"id: \" + self.id + \\\n",
    "            \", text: \" + self.text + \\\n",
    "            \", emotion: \" + self.emotion + \\\n",
    "            \", intensity: \" + self.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_training_data(training_data_file_path):\n",
    "\n",
    "    train_list = list()\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            train_list.append(Tweet(array[0], clean_str(array[1]), array[2], float(array[3])))\n",
    "    return train_list\n",
    "            \n",
    "def read_test_data(training_data_file_path):\n",
    "\n",
    "    test_list = list()\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            test_list.append(Tweet(array[0], clean_str(array[1]), array[2], None))\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion = \"joy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/dataset/\" + \\\n",
    "    emotion + \"-ratings-0to1.train.txt\"\n",
    "test_data_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/dataset/\" + \\\n",
    "    emotion + \"-ratings-0to1.dev.target.txt\"\n",
    "predictions_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/predictions/\" + \\\n",
    "    emotion + \"-pred.txt\"\n",
    "gold_set_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/dataset/gold-set/\" + \\\n",
    "    emotion + \"-ratings-0to1.dev.gold.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823\n"
     ]
    }
   ],
   "source": [
    "training_tweets = read_training_data(training_data_file_path)\n",
    "\n",
    "score_train = list()\n",
    "tweet_train = list()\n",
    "for tweet in training_tweets:\n",
    "    tweet_train.append(tweet.text)\n",
    "    score_train.append(float(tweet.intensity))\n",
    "    hashtag_emotion_intensities\n",
    "print(len(score_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "word_intensities = get_word_affect_intensity_dict(emotion)\n",
    "\n",
    "def get_emo_int_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in word_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(word_intensities[word])\n",
    "    \n",
    "    return poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(tweet):\n",
    "    vector_list = list()\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vector_list.append(wv_model[word])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if len(vector_list) == 0:\n",
    "        vec_rep = np.zeros(300).tolist()\n",
    "    else:\n",
    "        vec_rep = sum(vector_list) / float(len(vector_list))\n",
    "\n",
    "#     x_vector.extend(poly_2.fit_transform(np.array(vec_rep).reshape(1, -1))[0].tolist())\n",
    "    return vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = get_affect_presence_list(emotion)\n",
    "\n",
    "def get_sentiment_emotion_feature(tweet):\n",
    "    for word in word_list:\n",
    "        if word in tweet:\n",
    "            return [1.0]\n",
    "    \n",
    "    return [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "hashtag_emotion_intensities = get_hashtag_emotion_intensity(emotion)\n",
    "\n",
    "def get_hashtag_emotion_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in hashtag_emotion_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(hashtag_emotion_intensities[word])\n",
    "    \n",
    "    return poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def vectorize_tweets(tweet_list):\n",
    "    vectors = list()\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "        x_vector = list()\n",
    "        x_vector.extend(get_emo_int_vector(tweet))\n",
    "        x_vector.extend(get_word2vec_embedding(tweet))\n",
    "        x_vector.extend(get_sentiment_emotion_feature(tweet))\n",
    "        x_vector.extend(get_hashtag_emotion_vector(tweet))\n",
    "        vectors.append(x_vector)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823\n",
      "323\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorize_tweets(tweet_train)\n",
    "print(len(x_train))\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "test_tweets = read_test_data(test_data_file_path)\n",
    "tweet_test = list()\n",
    "for tweet in test_tweets:\n",
    "    tweet_test.append(tweet.text)\n",
    "\n",
    "x_test = vectorize_tweets(tweet_test)\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble, svm\n",
    "\n",
    "ml_model = ensemble.GradientBoostingRegressor(n_estimators=100)\n",
    "ml_model.fit(x_train, score_train)\n",
    "\n",
    "y_test = ml_model.predict(X=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(predictions_file_path, 'w') as predictions_file:\n",
    "    for i in range(len(y_test)):\n",
    "        predictions_file.write(\n",
    "            str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "            test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol thought maybe , could n't decide levity 0.373060087771\n",
      "nawaz sharif getting funnier day day laughter challenge kashmir baloch 0.552776455354\n",
      "nawaz sharif getting funnier day day challenge kashmir baloch 0.497147744763\n",
      "'ll people watch enjoy rare show optimism 0.486934470901\n",
      "love family much lucky grateful smartassfamily love 0.742502326897\n",
      "love family much lucky grateful smartassfamily hilarious love 0.746048511005\n",
      "assure laughter , increasing anger costs , arrogance westminster 0.390278405354\n",
      "trump supporters hillary haters wanna chirp weak minded , pandering liberals tweet 0.199214901771\n",
      "google caffeine sprightly lengthening corridor seo wgj 0.414636446228\n",
      "tweet dedicated back pain , understand youthful spry full life vivacious 0.563719379061\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tweet_test[i], y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation based on Pearson and Spearman co-efficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import scipy.stats\n",
    "\n",
    "def evaluate(pred,gold):\n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "        \n",
    "        for line in gold_lines:\n",
    "            line = line.decode()\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:   \n",
    "                data_dic[int(parts[0])]=[float(line.split('\\t')[3])]\n",
    "            else:\n",
    "                raise ValueError('Format problem.')\n",
    "        \n",
    "        \n",
    "        for line in pred_lines:\n",
    "            line = line.decode()\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:  \n",
    "                if int(parts[0]) in data_dic:\n",
    "                    try:\n",
    "                        data_dic[int(parts[0])].append(float(line.split('\\t')[3]))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[int(parts[0])].append(0.5)\n",
    "                else:\n",
    "                    raise ValueError('Invalid tweet id.')\n",
    "            else:\n",
    "                raise ValueError('Format problem.')\n",
    "            \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "         \n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                if(data_dic[id][0]>=0.5):\n",
    "                    gold_scores_range_05_1.append(data_dic[id][0])\n",
    "                    pred_scores_range_05_1.append(data_dic[id][1])\n",
    "            else:\n",
    "                raise ValueError('Repeated id in test data.')\n",
    "                \n",
    "      \n",
    "        # return zero correlation if predictions are constant\n",
    "        if numpy.std(pred_scores)==0 or numpy.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                    \n",
    "        spear_corr=scipy.stats.spearmanr(pred_scores,gold_scores)[0]   \n",
    "\n",
    "\n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                                    \n",
    "        spear_corr_range_05_1=scipy.stats.spearmanr(pred_scores_range_05_1,gold_scores_range_05_1)[0]           \n",
    "        \n",
    "      \n",
    "        return (pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1)\n",
    "                                           \n",
    "                          \n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Predictions and gold data have different number of lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n",
      "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\n",
      "(0.71351616580957089, 0.70154061079167207, 0.71644580899479782, 0.71107999104916364)\n"
     ]
    }
   ],
   "source": [
    "print(emotion)\n",
    "print(\"pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\")\n",
    "print(evaluate(predictions_file_path, gold_set_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Implementation in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu,device=cuda0\"    \n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n"
     ]
    }
   ],
   "source": [
    "# define base model\n",
    "_, dim_size = (np.array(x_train).shape)\n",
    "print(dim_size)\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10000, input_dim=dim_size, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3333, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(333, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(33, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=1000, batch_size=5, verbose=0)\n",
    "estimator.fit(x_train, score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "y_test = estimator.predict(x_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(predictions_file_path, 'w') as predictions_file:\n",
    "    for i in range(len(y_test)):\n",
    "        predictions_file.write(\n",
    "            str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "            test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overall Score estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "anger\n",
      "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\n",
      "0.574913084482 0.555563854364 0.421183111691 0.450580799362\n",
      "\n",
      "fear\n",
      "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\n",
      "0.594464880318 0.544222751852 0.534740205821 0.518208063738\n",
      "\n",
      "sadness\n",
      "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\n",
      "0.533999324202 0.527642090238 0.211824860544 0.162130602022\n",
      "\n",
      "joy\n",
      "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\n",
      "0.0603762913078 0.188182878887 0.00468160672095 0.0796393378405\n",
      "\n",
      "===============================\n",
      "\n",
      "Average Scores\n",
      "pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\n",
      "0.440938395077 0.453902893835 0.293107446194 0.302639700741\n"
     ]
    }
   ],
   "source": [
    "pears_corr_sum = 0\n",
    "spear_corr_sum = 0\n",
    "pears_corr_range_05_1_sum = 0\n",
    "spear_corr_range_05_1_sum = 0\n",
    "\n",
    "for emotion in ['anger', 'fear', 'sadness', 'joy']:\n",
    "    print(\"\\n\" + emotion)\n",
    "    predictions_file_path = \\\n",
    "        \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/predictions/\" + \\\n",
    "        emotion + \"-pred.txt\"\n",
    "    gold_set_path = \\\n",
    "        \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/dataset/gold-set/\" + \\\n",
    "        emotion + \"-ratings-0to1.dev.gold.txt\"\n",
    "    print(\"pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\")\n",
    "    pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1 = \\\n",
    "        evaluate(predictions_file_path, gold_set_path)\n",
    "    print(pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1)\n",
    "    pears_corr_sum += pears_corr\n",
    "    spear_corr_sum += spear_corr\n",
    "    pears_corr_range_05_1_sum += pears_corr_range_05_1\n",
    "    spear_corr_range_05_1_sum += spear_corr_range_05_1\n",
    "    \n",
    "print(\"\\n===============================\\n\")\n",
    "print(\"Average Scores\")\n",
    "print(\"pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1\")\n",
    "print(pears_corr_sum/4,spear_corr_sum/4,pears_corr_range_05_1_sum/4,spear_corr_range_05_1_sum/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
