{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build intial word2vec and Linear SVR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_path = \"/home/v2john/Documents/GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv_model = \\\n",
    "    gensim.models.KeyedVectors.\\\n",
    "    load_word2vec_format(\n",
    "        wv_model_path, binary=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/dataset/anger-ratings-0to1.train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input_data(training_data_file_path):\n",
    "\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            yield Tweet(array[0], clean_str(array[1]),\n",
    "                        array[2], float(array[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "\n",
    "    def __init__(self, id, text, emotion, intensity):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.emotion = emotion\n",
    "        self.intensity = intensity\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "            \"id: \" + self.id + \\\n",
    "            \", text: \" + self.text + \\\n",
    "            \", emotion: \" + self.emotion + \\\n",
    "            \", intensity: \" + self.intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(read_input_data(training_data_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list()\n",
    "y_train = list()\n",
    "for tweet in tweets:\n",
    "    split_text_list = tweet.text.split()\n",
    "\n",
    "    vector_list = list()\n",
    "    for word in split_text_list:\n",
    "        try:\n",
    "            vector_list.append(wv_model[word])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    sentence_vector = sum(vector_list) / float(len(vector_list))\n",
    "\n",
    "    x_train.append(sentence_vector)\n",
    "    y_train.append(tweet.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 857\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -18.94 (+/- 11.64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "scores = \\\n",
    "    model_selection.cross_val_score(\n",
    "        LinearSVR(), x_train, y_train, cv=10, scoring='r2'\n",
    "    )\n",
    "mean_score = scores.mean()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (mean_score, scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the scores for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_file_path = \\\n",
    "    \"/home/v2john/MEGA/Academic/Masters/UWaterloo/Research/WASSA-Task/dataset/anger-ratings-0to1.dev.target.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test_data(training_data_file_path):\n",
    "\n",
    "    with open(training_data_file_path) as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            array = line.split('\\t')\n",
    "            yield Tweet(array[0], clean_str(array[1]), array[2], array[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = list(read_test_data(test_data_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model = LinearSVR()\n",
    "ml_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = list()\n",
    "for tweet in test_tweets:\n",
    "    split_text_list = tweet.text.split()\n",
    "\n",
    "    vector_list = list()\n",
    "    for word in split_text_list:\n",
    "        try:\n",
    "            vector_list.append(wv_model[word])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    sentence_vector = sum(vector_list) / float(len(vector_list))\n",
    "    x_test.append(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = ml_model.predict(X=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10857\tpls dont insult the word 'molna'\tanger\t0.553656300935\n",
      "\n",
      "10858\ti would have almost took offense to this if i actually snapped you\tanger\t0.480488005981\n",
      "\n",
      "10859\tthat rutgers game was an abomination an affront to god and man we must never speak of it again\tanger\t0.764946162428\n",
      "\n",
      "10860\tthat 's what lisa asked before she started raging at me , 'can i call you \\? ' heh\tanger\t0.584031875066\n",
      "\n",
      "10861\tsometimes i get mad over something so minuscule i try to ruin somebodies life not like lose your job like get you into federal prison\tanger\t0.617129987709\n",
      "\n",
      "10862\tsometimes i get mad over something so minuscule i try to ruin somebodies life not like lose your job like get you into federal prison anger\tanger\t0.62258145163\n",
      "\n",
      "10863\ti think amp must actually have to be working like me amp because i havent got any snap chat videos today\tanger\t0.441040897392\n",
      "\n",
      "10864\tmy eyes have been dilated i hate the world right now with the rage of a thousand fiery dragons i need a drink\tanger\t0.354244853958\n",
      "\n",
      "10865\tone chosen by the clp members ! mp seats are not for people to dole out to their mates , we elect candidates fuming\tanger\t0.556010089365\n",
      "\n",
      "10866\tone chosen by the clp members ! mp seats are not for people to dole out to their mates , we elect candidates\tanger\t0.532001808559\n",
      "\n",
      "10867\tcan you please not have canadian players play us players , that lag is atrocious fixthisgame trash sfvrefund\tanger\t0.735188455833\n",
      "\n",
      "10868\ti love how theres no outrage that it 's a white man but if it was a black man them blm would be all over it regardless of reason\tanger\t0.573786162968\n",
      "\n",
      "10869\tme being on my dean really saving a lot of ppl , bc i do n't snap nomore amp it take so much out of me\tanger\t0.501521192292\n",
      "\n",
      "10870\tsorry guys i have absolutely no idea what time i 'll be on cam tomorrow but will keep you posted fuming\tanger\t0.527343280646\n",
      "\n",
      "10871\tsorry guys i have absolutely no idea what time i 'll be on cam tomorrow but will keep you posted\tanger\t0.504302483894\n",
      "\n",
      "10872\tis it me , or is ding wearing the look of a man who 's just found his arch enemy in bed with his missus \\? angryman\tanger\t0.523945608445\n",
      "\n",
      "10873\tis it me , or is ding wearing the look of a man who 's just found his arch enemy in bed with his missus \\? angryman scowl\tanger\t0.529282724215\n",
      "\n",
      "10874\ti fuck with madden way harder\tanger\t0.561153741765\n",
      "\n",
      "10875\tobama must be fuming lol\tanger\t0.7698078863\n",
      "\n",
      "10876\thate when guys cant control their anger\tanger\t0.716972583427\n",
      "\n",
      "10877\tshe would frown a bit , folding her arms 'why is it that every time i'm in need of assistance someone expects a lil\tanger\t0.482474025007\n",
      "\n",
      "10878\tliterally fuming fuck sake\tanger\t0.7954445022\n",
      "\n",
      "10879\toh so that 's where brian was ! where was my invite \\?\tanger\t0.580869303593\n",
      "\n",
      "10880\toh so that 's where brian was ! where was my invite \\? offended\tanger\t0.610273472254\n",
      "\n",
      "10881\tfrustration , looking up at elphaba in a frown of aggravation her high pitched voice was growing more and more\tanger\t0.418664907746\n",
      "\n",
      "10882\tit 's the most magical time of the year xmas party announced and the outrage commences gotta love silicon valley millennials\tanger\t0.432639640714\n",
      "\n",
      "10883\ti am sitting here wrapped in a fluffy blanket , with incense burning , listening to bon iver and drinking mulled wine i'm there\tanger\t0.215077696806\n",
      "\n",
      "10884\ttake 2k out of it the numbers on madden are low and have dropped and people are unhappy\tanger\t0.572906498703\n",
      "\n",
      "10885\ti would n't have anger issues if she did n't have lying issues think about that one pow lies confusion\tanger\t0.396195319505\n",
      "\n",
      "10886\ti will fight this guy ! do n't insult the lions like that ! but seriously they kinda are wasted some of the best players\tanger\t0.440207041073\n",
      "\n",
      "10887\teverybody talking about 'the first day of fall' but summer '16 is never gonna die revenge\tanger\t0.572708920763\n",
      "\n",
      "10888\tthe rage has died down\tanger\t0.475845687113\n",
      "\n",
      "10889\tananya just grabbed a bible , opened it , started reading , and then said 'where do they talk about burning people \\? '\tanger\t0.368175173276\n",
      "\n",
      "10890\tall brian does is sleep and aggravate me\tanger\t0.731173764101\n",
      "\n",
      "10891\ttrue nthey r burning with other 's pleasure ! nppl in true love happiness everywhere\tanger\t0.18153103111\n",
      "\n",
      "10892\tso in your opinion is this the worst delhi govt \\? acrid bitter hypocrisy\tanger\t0.626595753322\n",
      "\n",
      "10893\tso in your opinion is this the worst delhi govt \\? acrid hypocrisy\tanger\t0.64755129184\n",
      "\n",
      "10894\ti live and die for mchanzo honeymoon crashing and burning the second they move in together\tanger\t0.597717076366\n",
      "\n",
      "10895\tfor the first time in my madden career i just set up a farm account should make motm much easier\tanger\t0.541839300949\n",
      "\n",
      "10896\tb er off ncfc is a grudge match \\)\tanger\t0.589090540203\n",
      "\n",
      "10897\tmight just leave and aggravate bae\tanger\t0.515122195757\n",
      "\n",
      "10898\trealest ever , relentless ever , inevitable that i win\tanger\t0.506123962236\n",
      "\n",
      "10899\tkik to trade , have fun or a conversation \\( kik youraffair \\) kik kikme messageme textme pics trade tradepics dm bored\tanger\t0.134969093295\n",
      "\n",
      "10900\tkik to trade , have fun or a conversation \\( kik youraffair \\) kik kikme messageme textme pics trade tradepics dm snap bored\tanger\t0.140593273999\n",
      "\n",
      "10901\tyou 're so thirsty for the chance to disagree w the left , that you do n't even realize when something is an affront to your bigoted platform\tanger\t0.595982396914\n",
      "\n",
      "10902\ts o to the girl that just hit my car not only did she get lucky w no scratch but also from being spared the wrath of sleep deprived kait\tanger\t0.499584459302\n",
      "\n",
      "10903\the 's just too raging to type properly ha ha !\tanger\t0.490079110518\n",
      "\n",
      "10904\toow16 sting decent new song\tanger\t0.0901799751845\n",
      "\n",
      "10905\tok scrubbed hands 5 times before trying to put them in neyeballs burning n evenmoreblind accidentally scared the cat whilst screeching\tanger\t0.51062990021\n",
      "\n",
      "10906\tjust joined pottermore and was sorted into hufflepuff fuming\tanger\t0.789957539268\n",
      "\n",
      "10907\tjust joined pottermore and was sorted into hufflepuff\tanger\t0.750317233956\n",
      "\n",
      "10908\tdo n't ask , you do n't get apologies if i 've offended you all due respect alan , i think you 've been fed duff info\tanger\t0.522888185647\n",
      "\n",
      "10909\ti miss my gran singing rawhide , in her deep baritone growl\tanger\t0.380757872716\n",
      "\n",
      "10910\thaving a baby born too soon is lifechanging 6 years on and it feels like only yesterday sad happy angry emotionalrollercoaster\tanger\t0.441646492599\n",
      "\n",
      "10911\thaving a baby born too soon is lifechanging 6 years on and it feels like only yesterday sad happy emotionalrollercoaster\tanger\t0.403843580285\n",
      "\n",
      "10912\ti would always be honest but it 's great to feedback opinion to the brand do n't want to offend them bloghour\tanger\t0.281251783238\n",
      "\n",
      "10913\tno no i insist that you give me your best insult first\tanger\t0.540973913394\n",
      "\n",
      "10914\tcoincidentally watched ulzana 's raid last night brutally indignant filmmaking\tanger\t0.338379637569\n",
      "\n",
      "10915\tsaddest part of this whole mess is that all of this anger is misdirected they should march 2 the whitehouse\tanger\t0.645642634868\n",
      "\n",
      "10916\tnote to self stop laughing at things that offend you , it 's ok to get mad at people n notetoself offended mad upset\tanger\t0.522070086756\n",
      "\n",
      "10917\ti think our defense here at usc is playing well , we just need to fix a few things on offense and we can win the pac 12 this year'\tanger\t0.420957531824\n",
      "\n",
      "10918\teat my ass' is no longer an insult\tanger\t0.680818499069\n",
      "\n",
      "10919\twhy to have vanity sizes \\? now sizes s , xs \\( evenxxs sometimes \\) are too big , wtf \\? ! dear corporate jerks , lithuania did n't need this rant angry\tanger\t0.577817421588\n",
      "\n",
      "10920\twhy to have vanity sizes \\? now sizes s , xs \\( evenxxs sometimes \\) are too big , wtf \\? ! dear corporate jerks , lithuania did n't need this rant\tanger\t0.553699387378\n",
      "\n",
      "10921\ti turn the game on wanting to play madden , and before i load up a game i just turn it off now\tanger\t0.60266065257\n",
      "\n",
      "10922\ti 've always been fiery , but never radical\tanger\t0.392269006761\n",
      "\n",
      "10923\ttaking umbrage because jimmy carr claimed that bilbo baggins went to mordor on 8 out of 10 cats does countdown know your baggins' , mate\tanger\t0.38622197245\n",
      "\n",
      "10924\tanger , resentment , and hatred are the destroyer of your fortune today\tanger\t0.581062225727\n",
      "\n",
      "10925\tanger , resentment , and hatred are the destroyer of your fortune today '\tanger\t0.581062225727\n",
      "\n",
      "10926\tif i spend even 5 minutes with you and you already irritate me i seriously will bitch you out until you shut up\tanger\t0.638625012127\n",
      "\n",
      "10927\tsting is just too damn earnest for early morning listening\tanger\t0.414275318282\n",
      "\n",
      "10928\tsting is just too damn earnest for early morning listening sting\tanger\t0.330782118893\n",
      "\n",
      "10929\tjust seeing alex revells face gets me angry\tanger\t0.496432002514\n",
      "\n",
      "10930\tspent months arranging swap , new jobs etc , now i 've found out she 's been leading me on a merry dance fuming with her\tanger\t0.40552299893\n",
      "\n",
      "10931\tbeware the fury of a weak king\tanger\t0.358917564079\n",
      "\n",
      "10932\tgrowl ! ! !\tanger\t0.361847537989\n",
      "\n",
      "10933\t\\( sam \\) brown 's law never offend people with style when you can fake that , you have to break us in this island\tanger\t0.538912448942\n",
      "\n",
      "10934\tevent started ! everyone is getting ready to travel to the lake of rage , where everything glows\tanger\t0.501909910822\n",
      "\n",
      "10935\tin scotland , the right wingers are the most rabid anti nationalists socialists are mostly in favour\tanger\t0.526251493004\n",
      "\n",
      "10936\tpeople will always get offended everyone 's situation is different ! just because we have kids does n't mean we have to settle\tanger\t0.478492291223\n",
      "\n",
      "10937\ti try not to let my anger seep into reviews , but i resent having my time wasted on books like that time is precious\tanger\t0.470424060776\n",
      "\n",
      "10938\ti hope my hustle do n't offend nobody\tanger\t0.359101317989\n",
      "\n",
      "10939\tjust watched django unchained , other people may frown , but i titter in delight ! 2 5\tanger\t0.359743268203\n",
      "\n",
      "10940\tlol little things like that make me so angry x\tanger\t0.564971390441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_tweets)):\n",
    "    print(\n",
    "        str(test_tweets[i].id) + \"\\t\" + test_tweets[i].text + \"\\t\" +\n",
    "        test_tweets[i].emotion +\"\\t\" + str(y_test[i]) + \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
